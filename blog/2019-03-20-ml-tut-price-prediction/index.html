<!DOCTYPE html>
<html lang="en">
<!-- Beautiful Jekyll 5.0.0 | Copyright Dean Attali 2020 -->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  

  

  <title>Creating Neural Networks with Python/Keras/Tensorflow to Predict the Future</title>

  
  <meta name="author" content="Henry Pan">
  

  <meta name="description" content="Previous Knowledge Required Understand what is a neural network (NN) and how it works conceptually. Python Basic understanding of what derivatives/gradients are Goals In this tutorial, I will go over 3 different approaches of creating a NN that can predict the prices of a particular cryptocurrency pair (ETHBTC). This include...">

  

  
  <meta name="keywords" content="career,software,engineering,life,education,inspiration,machine learning,deep learning,reinforcement learning,web development,react">
  

  

  

  

  
<!-- Google Analytics -->
<script>
  (function (i, s, o, g, r, a, m) {
    i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
      (i[r].q = i[r].q || []).push(arguments)
    }, i[r].l = 1 * new Date(); a = s.createElement(o),
      m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
  })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');
  ga('create', 'UA-103622213-1', 'auto');
  ga('send', 'pageview');
</script>
<!-- End Google Analytics -->


  


  
    
      
  <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">


    
      
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800">


    
  

  
    
      <link rel="stylesheet" href="/blog/assets/css/bootstrap-social.css">
    
      <link rel="stylesheet" href="/blog/assets/css/beautifuljekyll.css">
    
  

  

  
  
  

  

  
  <meta property="og:site_name" content="Henry's Blog">
  <meta property="og:title" content="Creating Neural Networks with Python/Keras/Tensorflow to Predict the Future">
  <meta property="og:description" content="Previous Knowledge Required Understand what is a neural network (NN) and how it works conceptually. Python Basic understanding of what derivatives/gradients are Goals In this tutorial, I will go over 3 different approaches of creating a NN that can predict the prices of a particular cryptocurrency pair (ETHBTC). This include...">

  

  
  <meta property="og:type" content="article">
  <meta property="og:article:author" content="Henry Pan">
  <meta property="og:article:published_time" content="2019-03-20T11:50:00-04:00">
  <meta property="og:url" content="/blog/2019-03-20-ml-tut-price-prediction/">
  <link rel="canonical" href="/blog/2019-03-20-ml-tut-price-prediction/">
  

  
  <meta name="twitter:card" content="summary">
  
  <meta name="twitter:site" content="@">
  <meta name="twitter:creator" content="@">

  <meta property="twitter:title" content="Creating Neural Networks with Python/Keras/Tensorflow to Predict the Future">
  <meta property="twitter:description" content="Previous Knowledge Required Understand what is a neural network (NN) and how it works conceptually. Python Basic understanding of what derivatives/gradients are Goals In this tutorial, I will go over 3 different approaches of creating a NN that can predict the prices of a particular cryptocurrency pair (ETHBTC). This include...">

  

  


  

  

</head>


<body>

  


  <nav class="navbar navbar-expand-xl navbar-light fixed-top navbar-custom top-nav-regular"><a class="navbar-brand" href="/blog">Henry's Blog</a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  <div class="collapse navbar-collapse" id="main-navbar">
    <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a class="nav-link" href="/blog/tags">Topics</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="https://www.henrypan.com">Author's home</a>
          </li>
        <li class="nav-item">
          <a class="nav-link" id="nav-search-link" href="#" title="Search">
            <span id="nav-search-icon" class="fa fa-search"></span>
            <span id="nav-search-text">Search</span>
          </a>
        </li></ul>
  </div>

  

  

</nav>



<div id="beautifuljekyll-search-overlay">

  <div id="nav-search-exit" title="Exit search">✕</div>
  <input type="text" id="nav-search-input" placeholder="Search">
  <ul id="search-results-container"></ul>
  
  <script src="https://unpkg.com/simple-jekyll-search@latest/dest/simple-jekyll-search.min.js"></script>
  <script>
    var searchjson = '[ \
       \
        { \
          "title"    : "Peaking into the real game", \
          "category" : "careerlife", \
          "url"      : "/blog/2021-11-15-peaking-into-the-real-game/", \
          "date"     : "November 15, 2021" \
        }, \
       \
        { \
          "title"    : "Tic-tac-toe Self-Play", \
          "category" : "reinforcement-learningresearch", \
          "url"      : "/blog/2019-12-06-tic-tac-toe-selfplay/", \
          "date"     : "December  6, 2019" \
        }, \
       \
        { \
          "title"    : "OpenAI Gym - Acrobot-v1", \
          "category" : "reinforcement-learningresearch", \
          "url"      : "/blog/2019-12-03-acrobot/", \
          "date"     : "December  3, 2019" \
        }, \
       \
        { \
          "title"    : "OpenAI Gym - Pendulum-v0", \
          "category" : "reinforcement-learningresearch", \
          "url"      : "/blog/2019-11-05-pendulum/", \
          "date"     : "November  5, 2019" \
        }, \
       \
        { \
          "title"    : "OpenAI Gym - MountainCar-v0", \
          "category" : "reinforcement-learningresearch", \
          "url"      : "/blog/2019-11-04-mountain-car/", \
          "date"     : "November  4, 2019" \
        }, \
       \
        { \
          "title"    : "BrawlStars AI Series (Part 2) - Reinforcement Learning", \
          "category" : "reinforcement-learningcomputer visiongameresearch", \
          "url"      : "/blog/2019-04-25-Brawlstars-RL/", \
          "date"     : "April 25, 2019" \
        }, \
       \
        { \
          "title"    : "BrawlStars AI Series (Part 1)", \
          "category" : "machine-learningdeep learninggameresearch", \
          "url"      : "/blog/2019-04-20-Brawlstars-AI/", \
          "date"     : "April 20, 2019" \
        }, \
       \
        { \
          "title"    : "Creating a Policy Gradient (PG) Agent to Trade", \
          "category" : "reinforcement-learningdeep learningtradingresearch", \
          "url"      : "/blog/2019-04-04-pg-trading/", \
          "date"     : "April  4, 2019" \
        }, \
       \
        { \
          "title"    : "Creating Neural Networks with Python/Keras/Tensorflow to Predict the Future", \
          "category" : "machine-learningtutorial", \
          "url"      : "/blog/2019-03-20-ml-tut-price-prediction/", \
          "date"     : "March 20, 2019" \
        }, \
       \
        { \
          "title"    : "Asynchronous Advantage Actor Critic (A3C)-Reinforcement Learning -Laymens Explanation", \
          "category" : "reinforcement-learningconcept", \
          "url"      : "/blog/2019-02-27-a3c-rl-layments-explanation/", \
          "date"     : "February 27, 2019" \
        }, \
       \
        { \
          "title"    : "React Redux Intro", \
          "category" : "reactfront-endsoftware development", \
          "url"      : "/blog/2019-01-26-react-redux-intro/", \
          "date"     : "January 26, 2019" \
        }, \
       \
        { \
          "title"    : "Career Paths in Data Science/Machine Learning", \
          "category" : "careermachine learningdata scienceanalyst", \
          "url"      : "/blog/2019-01-10-career-paths-in-ml/", \
          "date"     : "January 10, 2019" \
        }, \
       \
        { \
          "title"    : "Looking back, planning forward", \
          "category" : "careercomputer sciencebusiness", \
          "url"      : "/blog/2015-12-03-looking-back-planning-forward/", \
          "date"     : "December  3, 2015" \
        }, \
       \
        { \
          "title"    : "A few thoughts on choosing a career path", \
          "category" : "careeradvice", \
          "url"      : "/blog/2013-12-24-a-few-thoughts-on-choosing-a-career-path/", \
          "date"     : "December 24, 2013" \
        }, \
       \
        { \
          "title"    : "Different Life Experiences Bring Different Perspectives", \
          "category" : "life", \
          "url"      : "/blog/2013-11-16-different-life-experiences-bring-different-perspectives/", \
          "date"     : "November 16, 2013" \
        }, \
       \
        { \
          "title"    : "Finding the &#39;right&#39; route", \
          "category" : "educationbusinesslife", \
          "url"      : "/blog/2013-09-25-finding-the-right-route/", \
          "date"     : "September 25, 2013" \
        }, \
       \
        { \
          "title"    : "Step Back and Rethink", \
          "category" : "careercomputer sciencebusiness", \
          "url"      : "/blog/2013-08-07-step-back-and-rethink/", \
          "date"     : "August  7, 2013" \
        }, \
       \
        { \
          "title"    : "One Sentence Summary of Books", \
          "category" : "books", \
          "url"      : "/blog/2013-07-18-one-sentence-summary-books/", \
          "date"     : "July 18, 2013" \
        }, \
       \
        { \
          "title"    : "1st day at McKinsey", \
          "category" : "corporate culture", \
          "url"      : "/blog/2013-05-15-first-day-at-mckinsey/", \
          "date"     : "May 15, 2013" \
        }, \
       \
        { \
          "title"    : "Dinner Talk", \
          "category" : "inspirationcareer", \
          "url"      : "/blog/2013-03-30-dinner-talk/", \
          "date"     : "March 30, 2013" \
        }, \
       \
        { \
          "title"    : "Movie Review - &#39;Accepted&#39;", \
          "category" : "movieeducation", \
          "url"      : "/blog/2013-03-16-movie-review-accepted/", \
          "date"     : "March 16, 2013" \
        }, \
       \
        { \
          "title"    : "Some Inspirational People", \
          "category" : "inspirationcareer", \
          "url"      : "/blog/2013-03-02-some-inspirational-people/", \
          "date"     : "March  2, 2013" \
        }, \
       \
       \
        { \
          "title"    : "Henry&#39;s Blog", \
          "category" : "page", \
          "url"      : "/blog/", \
          "date"     : "January 1, 1970" \
        }, \
       \
        { \
          "title"    : "Tag Index", \
          "category" : "page", \
          "url"      : "/blog/tags/", \
          "date"     : "January 1, 1970" \
        }, \
       \
        { \
          "title"    : "Henry&#39;s Blog", \
          "category" : "page", \
          "url"      : "/blog/page2/", \
          "date"     : "January 1, 1970" \
        }, \
       \
        { \
          "title"    : "Henry&#39;s Blog", \
          "category" : "page", \
          "url"      : "/blog/page3/", \
          "date"     : "January 1, 1970" \
        }, \
       \
        { \
          "title"    : "Henry&#39;s Blog", \
          "category" : "page", \
          "url"      : "/blog/page4/", \
          "date"     : "January 1, 1970" \
        }, \
       \
        { \
          "title"    : "Henry&#39;s Blog", \
          "category" : "page", \
          "url"      : "/blog/page5/", \
          "date"     : "January 1, 1970" \
        } \
       \
    ]';
    searchjson = JSON.parse(searchjson);

    var sjs = SimpleJekyllSearch({
      searchInput: document.getElementById('nav-search-input'),
      resultsContainer: document.getElementById('search-results-container'),
      json: searchjson
    });
  </script>
</div>





  <!-- TODO this file has become a mess, refactor it -->







<header class="header-section ">

<div class="intro-header no-img">
  <div class="container-md">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
        <div class="post-heading">
          <h1>Creating Neural Networks with Python/Keras/Tensorflow to Predict the Future</h1>
          

          
            <span class="post-meta">Posted on March 20, 2019</span>
            
            
          
        </div>
      </div>
    </div>
  </div>
</div>
</header>





<div class=" container-md ">
  <div class="row">
    <div class=" col-xl-8 offset-xl-2 col-lg-10 offset-lg-1 ">

      

      

      <article role="main" class="blog-post">
        <h2 id="previous-knowledge-required">Previous Knowledge Required</h2>

<ul>
  <li>Understand what is a neural network (NN) and how it works conceptually.</li>
  <li>Python</li>
  <li>Basic understanding of what derivatives/gradients are</li>
</ul>

<h2 id="goals">Goals</h2>
<p>In this tutorial, I will go over 3 different approaches of creating a NN that can predict the prices of a particular cryptocurrency pair (ETHBTC). This include using (very-low-level) Numpy/raw Python, (low-level) Tensorflow and (high-level) Keras.</p>

<p>Since it’s similar to predicting any price/number given a sequence of historical prices/numbers, I will describe this process as general as possible. The purpose of this tutorial is more about how to create NNs from scratch and to understand how high level frameworks like Keras work underneath the hood. It’s less about the correctness of predicting the future.</p>

<p><strong>Personal goal:</strong> When I was studying machine learning, I thought it would be good for me to implement things at the low level first, and then slowly move up the abstraction to improve productivity. I made sure that the 3 approaches all achieved the same outcome.</p>

<h2 id="showcase">Showcase</h2>

<p>Since the outcome of the 3 approaches are the same, I’ll just show one set of the training and testing result. All three sets are in the <a href="https://github.com/workofart/work-trader">repo</a>, and you can regenerate them if you’d like.</p>

<p><em>Note: The prices in the graph are normalized, but the accuracy is the same if denormalized. Again, this is just an illustration of how NN works and by no means a correct way to predict prices.</em></p>

<h4 id="training-set">Training Set</h4>
<p><img src="/blog/assets/images/ml/trainingset.png" width="600" /></p>

<h4 id="test-set">Test Set</h4>
<p><img src="/blog/assets/images/ml/testset.png" width="600" /></p>

<h2 id="input-data">Input Data</h2>
<p>82 Hours worth of BTCETH data in 10-second increments covering the following dimensions:</p>
<ul>
  <li>Closing price</li>
  <li>high</li>
  <li>low</li>
  <li>volume</li>
</ul>

<p>Source: Binance</p>

<h2 id="neural-network-architecture-all-3-versions">Neural Network Architecture (All 3 Versions)</h2>
<p><a name="nn-architecture"></a></p>

<p>3 Layers, <strong>Relu Activation Function</strong> for first (n-1) layers, with last layer being a <strong>linear output</strong>. The 1st hidden layer contains 16 neurons, the 2nd hidden layer contains 6 neurons. The <code class="language-plaintext highlighter-rouge">N</code> denotes the number of samples.</p>

<p>Note that when counting layers, we usually don’t count the layer without tunable parameters. In this case, the input layer doesn’t have tunable parameters, which results in a 3-layer NN, as opposed to a 4-layer NN.</p>

<p><img src="/blog/assets/images/ml/NN_architecture.png" alt="NN" /></p>

<h2 id="version-1">Version 1</h2>
<blockquote>
  <p>(Hand-coded Neural Network (without using any 3rd party framework)</p>
</blockquote>

<p><a href="https://github.com/workofart/work-trader/tree/master/v1">Code</a></p>

<p>In this version, we need to understand the innerworkings of NNs. In other words, how propagation of neuron computations take place and how to compute gradients from a programmatic perspective. I’ve borrowed and adapted some of the homework code from <a href="https://www.coursera.org/learn/neural-networks-deep-learning">Andrew Ng’s Coursera Course</a> on Deep Learning and Neural Networks to fit our context.</p>

<p><strong>1. Initialize parameters</strong></p>
<ul>
  <li>Neuron Weights (W)</li>
  <li>Bias Weights (B)</li>
</ul>

<p><strong>2. Define hyperparameters</strong></p>

<ul>
  <li>Learning Rate - how much each step of gradient descent should move</li>
  <li>Number of training iterations</li>
  <li>Number of hidden layers (Layers excluding input layer)</li>
  <li>
    <p>Activation function for each layer</p>

    <p>The dimensions of the NN is defined on this line: <code class="language-plaintext highlighter-rouge">layers_dims = [X_train.shape[0], 16, 6, Y_train.shape[0]]</code></p>

    <p>It means the <strong>first input layer</strong> takes in a size of <code class="language-plaintext highlighter-rouge">X_train.shape[0]</code>. In our example, that would be equal to <code class="language-plaintext highlighter-rouge">4</code> since there are 4 dimensions (Price, High, Low, Volume) for every data point. The <strong>first hidden layer</strong> (2nd element in the array) contains 16 neurons, <strong>second hidden layer</strong> contains 6 neurons, and the <strong>output layer</strong> contains <code class="language-plaintext highlighter-rouge">Y_train.shape[0]</code>, in our example that is equal to <code class="language-plaintext highlighter-rouge">1</code> since we’re predicting one price at a time.</p>

    <p>To summarize, the NN looks like <a href="#nn-architecture">this</a></p>
  </li>
</ul>

<p><strong>3. Define and perform training - loop for <code class="language-plaintext highlighter-rouge">num_iterations</code>:</strong>
<a name="nn-forward-def"></a></p>
<ul>
  <li>
    <p>Forward propagation</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  Usually one forward pass goes like this:

  Input -&gt; Matrix Multiplication (Linear) -&gt; Activation Function (Non-Linear)-&gt; 
  |_____________________ Repeat this N times (N Layers) ______________________|
</code></pre></div>    </div>

    <p>In our price prediction example (we use a linear output since we’re predicting values not classifying categories):
  [LINEAR-&gt;RELU]*(N-1)-&gt;LINEAR</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  Matrix Multiplication (Linear) = Input X * Weights + Bias
  Activation Function (Non-Linear) = Relu(Matrix Multiplication Result) = max(0, result)
</code></pre></div>    </div>
  </li>
  <li>
    <p>Compute cost function</p>

    <p>After we have performed one pass of our forward propagation, we will have obtained the predictions (from the last layer’s activation function output) and we can compare it with the ground truth to compute the cost. Note that I’m using MSE (Mean-squared Error), that’s a common cost function for value prediction. I’ll keep the notations consistent with the code so you can refer to it if necessary.</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  AL -- predicted "values" vector, shape (1, number of examples)
  Y -- true "values" vector, shape (1, number of examples)

  cost = (np.square(AL - Y)).mean(axis=1)
</code></pre></div>    </div>
  </li>
  <li>
    <p>Backward propagation</p>

    <p>After computing the cost, or how far off our predictions are from our true values, we can use that cost to adjust our weights in our NN. But first, we need to get the gradients of 3 things with respect to our cost: (1) Gradient of predicted Y value, (2) gradient of weights of each hidden unit, and (3) gradient of weights of the bias unit. With these gradients under our belt, we can know how to adjust our weights to minimize the cost.</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  One backward pass goes like this, the 3 gradients will be computed for each layer

  Cost -&gt;  Activation Function (Non-Linear)-&gt; Matrix Multiplication (Linear) -&gt;
  |_____________________ Repeat this N times (N Layers) ______________________|
</code></pre></div>    </div>
  </li>
  <li>
    <p>Update parameters (using parameters, and grads from backprop)</p>

    <p>At this stage, we have finished one back propagation and obtained all 3 types of gradients for all of our weights needed to adjust our NN.</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  parameters["W" + str(l + 1)] = parameters["W" + str(l + 1)] - learning_rate * grads["dW" + str(l + 1)]
  parameters["b" + str(l + 1)] = parameters["b" + str(l + 1)] - learning_rate * grads["db" + str(l + 1)]
</code></pre></div>    </div>

    <p>We’re simply doing:</p>

    <p><code class="language-plaintext highlighter-rouge">parameter = parameter - learning rate * gradient of that parameter</code></p>
  </li>
</ul>

<p><strong>4. Use trained parameters to predict prices</strong></p>

<p>We just perform a forward pass just like in training. It will produce the predicted values based on the current NN weights.</p>

<h2 id="version-2">Version 2</h2>
<blockquote>
  <p>Keras-based Neural Network</p>
</blockquote>

<p><a href="https://github.com/workofart/work-trader/tree/master/v2">Code</a></p>

<p>In this version, since we’re dealing with high-level Keras framework, we only need to have good idea of the architecture of the NN and how to construct it using the building blocks provided by Keras (just like lego). We don’t need to implement matrix multiplication or activation functions. We <strong>should</strong>, however, understand <em>how we initialize our weights, which activation functions to choose and how to structure our NN</em>. If you have time, you might even want to tweak the “icing on the cake” to prevent overfitting by applying regularization and dropout techniques. The reason I mention the “icing” here in version 2 and not in version 1 is because all of these components are lego pieces that you don’t need to implement yourself. This is why high-level frameworks provide a productivity boost over hand-coded solutions. But it’s always good to understand what’s going on under the hood to debug potential issues.</p>

<p><strong>In our example:</strong></p>

<ol>
  <li>
    <p>Instantiate a sequential model. This is like a container that holds the NN and its layers. Read more about <a href="https://keras.io/models/sequential/">Keras Sequential Models</a></p>

    <p><code class="language-plaintext highlighter-rouge">model = Sequential()</code></p>
  </li>
  <li>
    <p>Add a Layer to the NN, note that we don’t need separate functions for forward/backward propagation, we just think in terms of layers in the NN. Read more about <a href="https://keras.io/layers/core/">Keras Layers</a>. The <code class="language-plaintext highlighter-rouge">16</code> is the number of neurons in this layer, and we’re using <code class="language-plaintext highlighter-rouge">relu</code> as the activation function. Remember the building block argument I said before, in a high-level framework, we only need to <em>determine</em> what pieces we need to build the NN, as opposed to <em>implementing</em> them.</p>

    <p><code class="language-plaintext highlighter-rouge">model.add(Dense(16, input_dim=X_train.shape[1], activation='relu'))</code></p>

    <blockquote>
      <p>Note that this is equivalent to our <code class="language-plaintext highlighter-rouge">L_model_forward()</code> function and <code class="language-plaintext highlighter-rouge">L_model_backward()</code> combined in <strong>Version 1</strong> since we think in terms of <em>operations</em> in <strong>Version 1</strong>, and <em>layers</em> in <strong>Version 2</strong></p>
    </blockquote>
  </li>
  <li>
    <p>Similarly, we add another layer to the NN. The output space is N by 6 dimenions, where N is the number of samples, and the 6 is the number of neurons in this layer.</p>

    <p><code class="language-plaintext highlighter-rouge">model.add(Dense(6, activation='relu'))</code></p>
  </li>
  <li>Finally, we add our output layer to the NN. The output space (<code class="language-plaintext highlighter-rouge">Y_train.shape[1]</code>) in our example is 1, since we’re only predicting one price at a time.
    <blockquote>
      <p>The difference in using <code class="language-plaintext highlighter-rouge">.shape[1]</code> and <code class="language-plaintext highlighter-rouge">shape[0]</code> in the two versions is because in version 1, to follow Andrew Ng’s course notation, the samples are placed along columns <code class="language-plaintext highlighter-rouge">shape[1]</code> and the features (input/output dimension) are rows <code class="language-plaintext highlighter-rouge">shape[0]</code>. But in version 2, it’s the opposite, thus <code class="language-plaintext highlighter-rouge">Y_train.shape[1]</code> here denotes the output dimension.</p>
    </blockquote>

    <p><code class="language-plaintext highlighter-rouge">model.add(Dense(Y_train.shape[1]))</code></p>
  </li>
  <li>
    <p>After the network is fully constructed, we have to tell it how to train the NN. This involves specifying the <a href="https://keras.io/optimizers/">optimizer</a> for the NN as well as the <a href="https://keras.io/losses/">loss</a> function</p>

    <p><code class="language-plaintext highlighter-rouge">model.compile(optimizer=SGD(lr=0.03), loss='mse') # SGD = Stochastic Gradient Descent</code></p>
  </li>
</ol>

<h2 id="version-3">Version 3</h2>
<blockquote>
  <p>Tensorflow-based Neural Network</p>
</blockquote>

<p><a href="https://github.com/workofart/work-trader/tree/master/v3">Code</a></p>

<p>So we’ve seen creating operations from scratch in our <strong>Version 1</strong>, and using a high-level framework to create a “model” of our NN and just “fitting” it in <strong>Version 2</strong>. In <strong>Version 3</strong>, we have to switch our conceptual model of a NN a little bit again, because I have to introduce you to the concept of a <a href="https://en.wikipedia.org/wiki/Tensor">Tensor</a>. In my definition, it’s a wrapper or a building block that can encompass a variable, a constant, an operation, or any series of operations. We can connect tensors together by referencing them.</p>

<p>Let’s quickly go through our example and I’ll explain line by line with respect to how they relate to our <strong>Version 1</strong> and <strong>Version 2</strong> conceptual models.</p>

<ol>
  <li>We will start by defining our input variables:
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> input_x = tf.placeholder('float', [None, X_train_orig.shape[1]], name='input_x')
 input_y = tf.placeholder('float', [None, Y_train_orig.shape[1]], name='input_y')
</code></pre></div>    </div>

    <p>Note that this is a “placeholder”, which means before we feed in the actual input data, this tensor will be empty. The dimensions for this placeholder is None by <code class="language-plaintext highlighter-rouge">X/Y_train_orig.shape[1]</code>, this means it’s “<strong>any number</strong> of samples by <code class="language-plaintext highlighter-rouge">shape[1]</code> of features per sample”. The <code class="language-plaintext highlighter-rouge">name</code> is optional, but it helps later when we need to debug.</p>

    <blockquote>
      <p>The row/column vs samples/features notations are consistent with Version 2, where <code class="language-plaintext highlighter-rouge">shape[1]</code>(columns) are the features, and <code class="language-plaintext highlighter-rouge">shape[0]</code>(rows) are the samples</p>
    </blockquote>
  </li>
  <li>
    <p>Next, we will define some of the weights of our NN, namely our hidden unit weights and bias unit weights.</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> W1 = tf.Variable(tf.random_normal([X_train_orig.shape[1], 16]))
 B1 = tf.Variable(tf.zeros([16]))
</code></pre></div>    </div>
    <p>Note that these tensor types are “Variable”, which means they will “vary” during our training process. These are, by default, <a href="https://www.tensorflow.org/guide/variables">trainable variables</a>.</p>
  </li>
  <li>
    <p>We will define our linear function and activation function together in one line:</p>

    <p><code class="language-plaintext highlighter-rouge">layer1 = tf.nn.relu(tf.add(tf.matmul(input_x, W1), B1))</code></p>

    <p>I will leave out the definition for <code class="language-plaintext highlighter-rouge">layer2</code> and <code class="language-plaintext highlighter-rouge">output</code> layer since they are similar in nature.</p>

    <p>If we break this down and see each computation clearly, it’s equivalent to:</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> # Matrix Multiplication to get the linear result first

 mat_result = tf.matmul(input_x, W1)
	
 # Add the result to the bias units using Numpy broadcasting

 linear_result = tf.add(mat_result, B1)

 # Apply rectified linear unit activation to the linear function result

 layer1 = tf.nn.relu(linear_result)
</code></pre></div>    </div>

    <p>This is similar to our Version 1 definition, <a href="#nn-forward-def">here</a>.
 Note that we’re refering <code class="language-plaintext highlighter-rouge">W1</code> and <code class="language-plaintext highlighter-rouge">B1</code> varibles from our second step. This establishes the connection between tensors.</p>
  </li>
  <li>
    <p>Before we can train the network, we still need to define the loss functions and define how to optimize (train) it.</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> cost = tf.reduce_mean(tf.square(output - input_y))
 optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)
</code></pre></div>    </div>

    <p>Note that we’re still reference other tensors <code class="language-plaintext highlighter-rouge">output</code>, <code class="language-plaintext highlighter-rouge">input_y</code>, and <code class="language-plaintext highlighter-rouge">cost</code>. We can use the <code class="language-plaintext highlighter-rouge">tf.reduce_mean()</code> function to compute the MSE loss. And since Tensorflow has a built-in <code class="language-plaintext highlighter-rouge">AdamOptimizer</code>, we can just call it. This is similar to <strong>Version 2’s</strong> <code class="language-plaintext highlighter-rouge">optimizer=SGD()</code>.</p>
  </li>
  <li>
    <p>Now we have finished defining all the tensors. It’s time to actually feed in the input data and see how the data flow through all the connected tensors.</p>

    <p>Initialize all the variables that are <strong>not</strong> placeholders, such as weights and biases</p>

    <p><code class="language-plaintext highlighter-rouge">init = tf.global_variables_initializer()</code></p>

    <p>Feed in our <code class="language-plaintext highlighter-rouge">batch_x</code> and <code class="language-plaintext highlighter-rouge">batch_y</code> inputs to the <strong>placeholders</strong>. Note that the names (keys) must match the variable names <code class="language-plaintext highlighter-rouge">input_x/y</code> and specify what we want to be returned: <code class="language-plaintext highlighter-rouge">optimizer</code>, and <code class="language-plaintext highlighter-rouge">cost</code> from step (4).</p>
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> _, c = sess.run([optimizer, cost], feed_dict={
         input_x: batch_x, 
         input_y: batch_y, 
     })
</code></pre></div>    </div>
  </li>
</ol>

<p><img src="/blog/assets/images/ml/tensorflow.png" /></p>
<h6 id="image-from-httpsplaygroundtensorfloworg">Image from https://playground.tensorflow.org/</h6>

<p>As you can see now, after we feed in the input data into the NN, all the connected tensors will subsequently receive the input from the previous output and perform their computations accordingly, thus the name <strong>“TensorFlow”</strong>.</p>

<hr />

<p>I will be posting another note for applying reinforcement learning to trading. Since even with predicted prices, the agent will still not know when to buy or sell (i.e. after a 1% price drop? 2%?). We don’t want to hard-code those conditions, rather we want the agent to learn them as the “policy”. Until next time…Thanks!</p>

      </article>

      
        <div class="blog-tags">
          <span>Tags:</span>
          
            <a href="/blog/tags#machine-learning">machine-learning</a>
          
            <a href="/blog/tags#tutorial">tutorial</a>
          
        </div>
      

      

      
        <!-- Check if any share-links are active -->





      

      <ul class="pagination blog-pager">
        
        <li class="page-item previous">
          <a class="page-link" href="/blog/2019-02-27-a3c-rl-layments-explanation/" data-toggle="tooltip" data-placement="top" title="Asynchronous Advantage Actor Critic (A3C)-Reinforcement Learning -Laymens Explanation">&larr; Previous Post</a>
        </li>
        
        
        <li class="page-item next">
          <a class="page-link" href="/blog/2019-04-04-pg-trading/" data-toggle="tooltip" data-placement="top" title="Creating a Policy Gradient (PG) Agent to Trade">Next Post &rarr;</a>
        </li>
        
      </ul>
      
  
  
  

  


  



    </div>
  </div>
</div>


  <footer>
  <div class="container-md beautiful-jekyll-footer">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
      <ul class="list-inline text-center footer-links"><li class="list-inline-item">
    <a href="mailto:hanxiangp@gmail.com" title="Email me">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Email me</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://github.com/workofart" title="GitHub">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-github fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">GitHub</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://linkedin.com/in/pan-henry" title="LinkedIn">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-linkedin fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">LinkedIn</span>
   </a>
  </li></ul>

      
      <p class="copyright text-muted">
      
        Henry Pan
        &nbsp;&bull;&nbsp;
      
      2021

      

      
      </p>
      <p class="theme-by text-muted">
        Powered by
        <a href="https://beautifuljekyll.com">Beautiful Jekyll</a>
      </p>
      </div>
    </div>
  </div>
</footer>


  
  
    
  <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=" crossorigin="anonymous"></script>


  
    
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>


  
    
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>


  



  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
      <script src="/blog/assets/js/beautifuljekyll.js"></script>
    
  





  
    
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


  





</body>
</html>
