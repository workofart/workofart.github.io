<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>OpenAI Gym - MountainCar-v0 | Henry’s Blog</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="OpenAI Gym - MountainCar-v0" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Code" />
<meta property="og:description" content="Code" />
<link rel="canonical" href="http://henrypan.com/blog/reinforcement-learning/2019/11/04/mountain-car.html" />
<meta property="og:url" content="http://henrypan.com/blog/reinforcement-learning/2019/11/04/mountain-car.html" />
<meta property="og:site_name" content="Henry’s Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-11-04T18:00:00-05:00" />
<script type="application/ld+json">
{"url":"http://henrypan.com/blog/reinforcement-learning/2019/11/04/mountain-car.html","headline":"OpenAI Gym - MountainCar-v0","datePublished":"2019-11-04T18:00:00-05:00","dateModified":"2019-11-04T18:00:00-05:00","description":"Code","mainEntityOfPage":{"@type":"WebPage","@id":"http://henrypan.com/blog/reinforcement-learning/2019/11/04/mountain-car.html"},"@type":"BlogPosting","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://henrypan.com/blog/feed.xml" title="Henry's Blog" /></head>
<body><header class="site-header" role="banner">
    <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Henry&#39;s Blog</a><nav class="site-nav">
          <input type="checkbox" id="nav-trigger" class="nav-trigger" />
          <label for="nav-trigger">
            <span class="menu-icon">
              <svg viewBox="0 0 18 15" width="18px" height="15px">
                <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
              </svg>
            </span>
          </label>
  
          <div class="trigger">
            <a class="page-link" href="http://henrypan.com">Homepage</a>
            <!---->
          </div>
        </nav></div>
  </header>
  <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">OpenAI Gym - MountainCar-v0</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2019-11-04T18:00:00-05:00" itemprop="datePublished">Nov 4, 2019
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h2 id="code">Code</h2>

<p><a href="https://github.com/workofart/openai-gym-baselines/tree/master/MountainCarContinuous-v0">Here</a></p>

<h2 id="1-goal">1. Goal</h2>
<p>The problem setting is to solve the <a href="https://gym.openai.com/envs/MountainCarContinuous-v0/">Continuous MountainCar</a> problem in OpenAI gym.</p>

<p><img src="https://github.com/workofart/openai-gym-baselines/raw/master/MountainCarContinuous-v0/test-run.gif" alt="test-run" />
<br /></p>

<h2 id="2-environment">2. Environment</h2>

<p>The mountain car follows a continuous state space as follows(copied from <a href="https://github.com/openai/gym/wiki/MountainCarContinuous-v0">wiki</a>):</p>

<p>The acceleration of the car is controlled via the application of a force which takes values in the range [1, 1]. The states are the position of the car in the horizontal axis on the range [1.2, 0.6] and its velocity on the range [0.07, 0.07]. The goal is to get the car to accelerate up the hill and get to the flag.</p>

<p><br /></p>

<h4 id="state">State</h4>

<table>
  <thead>
    <tr>
      <th>Num</th>
      <th>Observation</th>
      <th>Min</th>
      <th>Max</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>Car Position</td>
      <td>-1.2</td>
      <td>0.6</td>
    </tr>
    <tr>
      <td>1</td>
      <td>Car Velocity</td>
      <td>-0.07</td>
      <td>0.07</td>
    </tr>
  </tbody>
</table>

<p>Note that velocity has been constrained to facilitate exploration, but this constraint might be relaxed in a more challenging version.</p>

<p><br /></p>

<h4 id="actions">Actions</h4>

<table>
  <thead>
    <tr>
      <th>Num</th>
      <th>Action</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>Push car to the left (negative value) or to the right (positive value)</td>
    </tr>
  </tbody>
</table>

<p><br /></p>

<h4 id="reward">Reward</h4>

<p>Reward is <code class="highlighter-rouge">100</code> for reaching the target of the hill on the right hand side, minus the squared sum of actions from start to goal.</p>

<p>This reward function raises an exploration challenge, because if the agent does not reach the target soon enough, it will figure out that it is better not to move, and won’t find the target anymore.</p>

<p>Note that this reward is unusual with respect to most published work, where the goal was to reach the target as fast as possible, hence favouring a bang-bang strategy.</p>

<p><br /></p>

<h4 id="starting-state">Starting State</h4>

<p>Position between <code class="highlighter-rouge">-0.6</code> and <code class="highlighter-rouge">-0.4</code>, null velocity.</p>

<p><br /></p>

<h4 id="episode-termination">Episode Termination</h4>

<p>Position equal to <code class="highlighter-rouge">0.5</code>. A constraint on velocity might be added in a more challenging version.</p>

<p>The episode will terminate either when the car has reached the goal OR when the total number of time steps reached 1000 regardless of reaching the goal or not.</p>

<p><br /></p>

<h4 id="solved-requirements">Solved Requirements</h4>

<p>Get a reward over <code class="highlighter-rouge">90</code>. This value might be tuned.</p>

<p><br /></p>

<h2 id="3-approach">3. Approach</h2>

<p>The approach uses the <em>policy gradient</em> algorithm with a baseline to reduce variance. Even though the state space is continuous, in this attempt, we will be using a discrete softmax policy. In other words, the continuous state space will be discretized into buckets of states that will be fed to the agent that will output a discrete action either <code class="highlighter-rouge">[-1, 0, 1]</code>, which is <code class="highlighter-rouge">[left, no-action, right]</code>.</p>

<p><br /></p>

<h3 id="31-discretization">3.1 Discretization</h3>

<p>Since there are two dimensions in the state space, namely position and velocity. We will discretize them separately into <strong>150</strong> buckets and <strong>120</strong> buckets for position and velocity respectively.</p>

<p><br /></p>

<h3 id="32-exploration-vs-exploitation">3.2 Exploration vs Exploitation</h3>

<p>To overcome the exploration-exploitation dilemma, we will be using the epsilon-greedy approach to slowly decrease the randomization factor overtime. This will ensure that our agent will have a wide variety of state-action training samples and in the later part of the training, it will allow the agent to follow it’s own “trained strategy” as opposed to random actions.</p>

<p>Technically, in the code, we will be using a <strong>temperature</strong> term to smooth the probability of actions, and <strong>epsilon</strong> to decide between whether to take a random action or the predicted action output from the policy.</p>

<p><br /></p>

<h3 id="33-training">3.3 Training</h3>

<p><strong>Monte Carlo</strong></p>

<p>The training process follows a <em>Monte Carlo</em> method. This means the training only takes place after an entire episode is completed, and replays the accumulated state/action/reward/next state for training. This is at one end of the spectrum, the other end of the spectrum is called <em>1-Step Temporal Difference</em> learning. So Monte Carlo is essentially a <em><script type="math/tex">\infty</script>-step Temporal Difference</em> learning. There is a balance between when you want to train the agent. Training it too early could render very messy results and thus might be harder to converge. Training it too late might prolong the training duration for convergence. <strong>The criteria for choosing the method depends heavily on the problem itself.</strong></p>

<p><img src="/blog/assets/images/rl/td_vs_montecarlo.png" width="800" /></p>

<center>[1] Richard S. Sutton and Andrew G. Barto. 2018. *Reinforcement Learning: An Introduction*. A Bradford Book, Cambridge, MA, USA.</center>

<p><br /></p>

<p><strong>Policy Gradient Weight Update</strong>
<script type="math/tex">\alpha_v = \text{Value function learning rate}\\
\alpha_p = \text{Policy function learning rate}\\
\hat{v} = \text{Estimated value for the given state} \\
\theta_v = \text{Value function parameterization weights}\\
\theta_p = \text{Policy function parameterization weights}\\
\delta = \text{Advantage}\\
\gamma = \text{Discount rate}\\\
G = \text{Discounted Rewards}</script></p>

<script type="math/tex; mode=display">\Large
\delta \leftarrow G - \hat{v}(S, \theta_{v})\\
\Large
\theta_{v} \leftarrow \theta_{v} + \alpha_v \delta \triangledown \hat{v}(S, \theta_{v}) \\
\Large
\theta_{p} \leftarrow \theta_{p} + \alpha_p \delta \gamma \triangledown \ln{\pi(A \mid S, \theta_p)}\\</script>

<p><strong>Gradient Calculation for Softmax Policy</strong>
<script type="math/tex">a = \text{action}\\
a' = \text{selected action by policy}\\
w = \text{policy weights}\\
\tau = \text{temperature for epsilon-greedy}\\
s = \text{current state} \\
\pi(a \mid s) = \text{policy outputs an action given the current state} \\
\Large
\quad \quad \quad = \frac{e^{w_{s,a}}}{\sum_{a'}e^{w_{s,a'}}}</script></p>

<script type="math/tex; mode=display">% <![CDATA[
\Large
\begin{align*}
    \triangledown_w \log \pi(a \mid s) &= \frac{\partial \log \frac{e^{w_{s,a}} / \tau}{\sum_{a'}e^{w_{s,a'}} / \tau}}{\partial w} \\
    \newline \\
    \text{(Take logs)} \; &= \frac{\partial \log \big(e^{w_{s,a}} / \tau\big) - \partial \log \big (\sum_{a'}e^{w_{s,a'}} / \tau \big)} {\partial w}\\
    \newline \\
    \text{(Chain & Log rule)} &= \frac{e^{w_{s, a}} / \tau  w_{s,a} / \tau}{e^{w_{s,a}} / \tau \ln{e}} - \frac{\sum_{a'} w_{s,a'} / \tau \; e^{w_{s,a'}/ \tau}}{\sum_{a'}e^{w_{s,a'}}/ \tau}\\
    \newline \\
    \text{(Simplify first equation)} \; &= w_{s,a} / \tau - \frac{\sum_{a'} w_{s,a'} / \tau \; e^{w_{s,a'}/ \tau}}{\sum_{a'}e^{w_{s,a'}}/ \tau}\\
    \newline \\
    % \text{(Second equation substitute } \pi(a \mid s) \text{)} \; &= w_{s,a} / \tau - \sum_{a'} w_{s,a'} / \tau \; \pi(a' \mid s)\\
    \text{(If Action = $a^\prime$) } &= (1 - \frac{e^{w_{s, a}}}{\sum_{a'}e^{w_{s,a'}}}) / \tau \\
    &= \frac{1 - \pi(a \mid s)}{\tau} \\
    \text{(If Action  $\neq a^\prime$) } &= -(\frac{e^{w_{s, a}}}{\sum_{a'}e^{w_{s,a'}}}) / \tau \\
    &= - \frac{\pi(a \mid s)}{\tau} \\
\end{align*} %]]></script>

<h3 id="34-hyperparameters">3.4 Hyperparameters</h3>

<ul>
  <li>Discount rate (<script type="math/tex">\gamma</script>): 0.999</li>
  <li>Value function learning rate (<script type="math/tex">\alpha_v</script>): <script type="math/tex">1e^{-2}</script></li>
  <li>Policy function learning rate (<script type="math/tex">\alpha_p</script>): <script type="math/tex">1e^{-3}</script></li>
  <li>Temperature (<script type="math/tex">\tau</script>): 1 decreasing to 0.5 linearly over all training episodes</li>
  <li>Discretization for position state: 150 buckets</li>
  <li>Discretization for velocity state: 120 buckets</li>
  <li>Epsilon-greedy (<script type="math/tex">\epsilon</script>): 1 decreasing to 0.1 linearly over all training episodes</li>
</ul>

<p><br /></p>

<h2 id="4-experiment--findings">4. Experiment &amp; Findings</h2>

<table>
  <thead>
    <tr>
      <th># Episodes</th>
      <th>Training Time</th>
      <th>Min. Reward</th>
      <th>Max Reward</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>2500</td>
      <td>473 seconds</td>
      <td>31.7</td>
      <td>90.6</td>
    </tr>
    <tr>
      <td>1500</td>
      <td>327 seconds</td>
      <td>31.5</td>
      <td>89.9</td>
    </tr>
    <tr>
      <td>1000</td>
      <td>254 seconds</td>
      <td>31.2</td>
      <td>89.3</td>
    </tr>
    <tr>
      <td>500</td>
      <td>154 seconds</td>
      <td>30.2</td>
      <td>86.7</td>
    </tr>
    <tr>
      <td>200</td>
      <td>77 seconds</td>
      <td>31.1</td>
      <td>84.6</td>
    </tr>
  </tbody>
</table>

<p><br /></p>

<h3 id="41-introducing-baseline-to-reduce-variance">4.1 Introducing baseline to reduce variance</h3>

<center><b>Without Baseline</b></center>

<p><img src="https://github.com/workofart/openai-gym-baselines/raw/master/MountainCarContinuous-v0/without_baseline.png" alt="without_baseline" style="zoom: 67%;" /></p>

<center>Training took: 280.78 seconds</center>

<hr />

<center><b>With Baseline</b></center>

<p><img src="https://github.com/workofart/openai-gym-baselines/raw/master/MountainCarContinuous-v0/with_baseline.png" alt="with_baseline" style="zoom:67%;" /></p>

<center>Training took: 253.72 seconds</center>

<p>The above comparison experiment is to show the effects of having a baseline on the agent’s performance. We can easily see from the rewards graph, the agent <strong>with baseline</strong> has a smaller variance in the rewards. This translates to a faster convergence rate.</p>

<p><br /></p>

<h3 id="42-discrete-vs-continuous-actions">4.2 Discrete vs Continuous Actions</h3>

<p>This problem (MountainCarContinuous-v0) was intended to be solved using a continuous action policy. However, I didn’t use continuous actions because I wanted to see how well a discrete-action agent could perform on this simple task. In conclusion, the number of episodes until convergence is quite good. The overall max reward reaches the goal of 90 at around ~1000 episodes.</p>

<p><br /></p>

<h3 id="43-performance">4.3 Performance</h3>

<p>Let’s look at the leaderboard of this problem and <strong>MountainCar-v0</strong>, which is a discrete version of the problem.</p>

<table>
  <thead>
    <tr>
      <th>MountainCar-v0</th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>User</strong></td>
      <td><strong>Episodes before solve</strong></td>
    </tr>
    <tr>
      <td><a href="https://github.com/ZhiqingXiao">Zhiqing Xiao</a></td>
      <td>0 (use close-form preset policy)</td>
    </tr>
    <tr>
      <td><a href="https://github.com/StepNeverStop">Keavnn</a></td>
      <td>47</td>
    </tr>
    <tr>
      <td><a href="https://github.com/ZhiqingXiao">Zhiqing Xiao</a></td>
      <td>75</td>
    </tr>
    <tr>
      <td><a href="https://github.com/roboticist-by-day">Mohith Sakthivel</a></td>
      <td>90</td>
    </tr>
    <tr>
      <td><a href="https://github.com/amohamed11">Anas Mohamed</a></td>
      <td>341</td>
    </tr>
    <tr>
      <td><a href="https://github.com/harshitandro">Harshit Singh Lodha</a></td>
      <td>643</td>
    </tr>
    <tr>
      <td><a href="https://github.com/CM-Data">Colin M</a></td>
      <td>944</td>
    </tr>
    <tr>
      <td><a href="https://github.com/jing582">jing582</a></td>
      <td>1119</td>
    </tr>
    <tr>
      <td><a href="https://github.com/DaveLeongSingapore">DaveLeongSingapore</a></td>
      <td>1967</td>
    </tr>
    <tr>
      <td><a href="https://github.com/Pechckin">Pechckin</a></td>
      <td>30</td>
    </tr>
    <tr>
      <td><a href="https://github.com/amitkvikram">Amit</a></td>
      <td>1000-1200</td>
    </tr>
    <tr>
      <td><a href="https://github.com/elcrion/mountain_car">Gleb I</a></td>
      <td>100</td>
    </tr>
  </tbody>
</table>

<table>
  <thead>
    <tr>
      <th>MountainCarContinuous-v0</th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>User</td>
      <td>Episodes before solve</td>
    </tr>
    <tr>
      <td><a href="https://github.com/Ashioto">Ashioto</a></td>
      <td>1</td>
    </tr>
    <tr>
      <td><a href="https://github.com/StepNeverStop">Keavnn</a></td>
      <td>11</td>
    </tr>
    <tr>
      <td><a href="https://github.com/camigord">camigord</a></td>
      <td>18</td>
    </tr>
    <tr>
      <td><a href="https://github.com/tobiassteidle">Tobias Steidle</a></td>
      <td>32</td>
    </tr>
    <tr>
      <td><a href="https://github.com/lirnli">lirnli</a></td>
      <td>33</td>
    </tr>
    <tr>
      <td><a href="https://github.com/Khev/RL-practice-keras/blob/master/DDPG/writeup_for_openai.ipynb">khev</a></td>
      <td>130</td>
    </tr>
    <tr>
      <td><a href="https://github.com/sanketsans/openAIenv/tree/master/CEM/mountainCar_Cont">Sanket Thakur</a></td>
      <td>140</td>
    </tr>
    <tr>
      <td><a href="https://github.com/Pechckin">Pechckin</a></td>
      <td>1</td>
    </tr>
    <tr>
      <td><a href="https://github.com/nikhilbarhate99">Nikhil Barhate</a></td>
      <td>200 (HAC)</td>
    </tr>
  </tbody>
</table>

<p>This shows that our discrete policy is a good baseline to start with before we dive into continuous actions. With continuous actions, debugging might be slightly harder as the policy function will be slightly more complicated, perhaps using a neural network approximating the policy function.</p>

<p><br /></p>

<h2 id="5-next-steps">5. Next Steps</h2>

<p>An obvious next step would be to try out continuous actions on this same problem and see how much more “efficient” and “effective” training can be. Intuitively, with continuous actions, you would expect a more fine-grain control of the car, and thus less “bouncing” around. Think driving with full petal and full brake, it’s not a very effective way to drive, is it?</p>

<p>Another thing that could be interesting to try out is to change the “n” in the <em>n-step Temporal Difference (TD)</em> learning and see the effects with regards to performance, convergence rate, reward variance. Right now we are using <em>Monte Carlo</em>, which is essentially a <script type="math/tex">\infty</script>-step TD method.</p>

<p>The current approach uses policy gradient as the approach to train the agent. There are some limitations in policy-based methods. There are other reinforcement learning algorithms that can be used to tackle this problem such as Deep Q-Learning, and Actor-Critic. I will be tackling another OpenAI Gym problem (<a href="https://github.com/openai/gym/wiki/Pendulum-v0">Pendulum-v0</a>) using actor-critic. Stay tuned for my next post.</p>


  </div><a class="u-url" href="/blog/reinforcement-learning/2019/11/04/mountain-car.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Henry&#39;s Blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Henry&#39;s Blog</li><li><a class="u-email" href="mailto:hanxiangp@gmail.com">hanxiangp@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/workofart"><svg class="svg-icon"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg> <span class="username">workofart</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>My learning notes, research, projects and musings.</p>
      </div>
    </div>

  </div>

</footer>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>
  </body>

</html>
