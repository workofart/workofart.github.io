<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Asynchronous Advantage Actor Critic (A3C)-Reinforcement Learning -Laymens Explanation | Henry’s Blog</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Asynchronous Advantage Actor Critic (A3C)-Reinforcement Learning -Laymens Explanation" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The A3C method in Reinforcement Learning (RL) combines both a critic’s value function (how good a state is) and an actor’s policy (a set of action probability for a given state). I promise this explanation doesn’t not contain greek letters or calculus. It only contains English alphabets and subtraction in math." />
<meta property="og:description" content="The A3C method in Reinforcement Learning (RL) combines both a critic’s value function (how good a state is) and an actor’s policy (a set of action probability for a given state). I promise this explanation doesn’t not contain greek letters or calculus. It only contains English alphabets and subtraction in math." />
<link rel="canonical" href="http://henrypan.com/blog/reinforcement-learning/2019/02/27/a3c-rl-layments-explanation.html" />
<meta property="og:url" content="http://henrypan.com/blog/reinforcement-learning/2019/02/27/a3c-rl-layments-explanation.html" />
<meta property="og:site_name" content="Henry’s Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-02-27T12:00:00-05:00" />
<script type="application/ld+json">
{"description":"The A3C method in Reinforcement Learning (RL) combines both a critic’s value function (how good a state is) and an actor’s policy (a set of action probability for a given state). I promise this explanation doesn’t not contain greek letters or calculus. It only contains English alphabets and subtraction in math.","@type":"BlogPosting","url":"http://henrypan.com/blog/reinforcement-learning/2019/02/27/a3c-rl-layments-explanation.html","headline":"Asynchronous Advantage Actor Critic (A3C)-Reinforcement Learning -Laymens Explanation","dateModified":"2019-02-27T12:00:00-05:00","datePublished":"2019-02-27T12:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://henrypan.com/blog/reinforcement-learning/2019/02/27/a3c-rl-layments-explanation.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://henrypan.com/blog/feed.xml" title="Henry's Blog" /></head>
<body><header class="site-header" role="banner">
    <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Henry&#39;s Blog</a><nav class="site-nav">
          <input type="checkbox" id="nav-trigger" class="nav-trigger" />
          <label for="nav-trigger">
            <span class="menu-icon">
              <svg viewBox="0 0 18 15" width="18px" height="15px">
                <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
              </svg>
            </span>
          </label>
  
          <div class="trigger">
            <a class="page-link" href="http://henrypan.com">Homepage</a>
            <!---->
          </div>
        </nav></div>
  </header>
  <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Asynchronous Advantage Actor Critic (A3C)-Reinforcement Learning -Laymens Explanation</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2019-02-27T12:00:00-05:00" itemprop="datePublished">Feb 27, 2019
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>The A3C method in Reinforcement Learning (RL) combines both a <em>critic’s value function</em> (how good a state is) and an <em>actor’s policy</em> (a set of action probability for a given state). <strong>I promise this explanation doesn’t not contain greek letters or calculus. It only contains English alphabets and subtraction in math.</strong></p>

<p><img src="/blog/assets/images/rl/a3c_architecture.png" alt="Diagram" /></p>

<h6 id="taken-from-hands-on-reinforcement-learning-with-python-by-sudharsan-ravichandiran">Taken from “Hands-On Reinforcement Learning with Python by Sudharsan Ravichandiran”</h6>

<p>Advantage in A3C is used to determine which actions were “good” and “bad”, and it is updated to encourage or discourage accordingly. Note that this also informs the agent how much better it is than expected. This is better than just using discounted rewards in vanilla Deep Q-learning. To see why, below is a formal explanation.</p>

<p>The advantage is estimated using <em>discounted rewards</em> (R) and the value from the <em>critic’s value function</em>, how good a state is, V(s). Thus formally:</p>

<blockquote>
  <p>Estimated Advantage = R-V(s)</p>
</blockquote>

<p>In A3C, there is a global network. This network will consist of a neural network to process the input data (states), and the output layers consists of value (how good a state is) and policy (a set of action probability for a given state) estimations.</p>

<p>The following summarizes the process of each episode:</p>
<ol>
  <li>To start the process, each worker initializes its network parameters equal to the global network.</li>
  <li>Each worker interacts with its own environment and accumulates experience in the form of tuples <em>(observation, action, reward, done, value)</em> after every interaction.</li>
  <li>Once the worker’s experience history reaches our set size, we calculate the <em>discounted return -&gt; estimated advantage -&gt; temporal difference (TD) -&gt;value and policy losses</em>. Note that we also calculate an entropy of the policy to understand the spread of the action probabilities. In other words, a high entropy is the result of similar action probabilities, or <em>uncertain what to do</em> in laymens terms. A low entropy means the agent is very confident (high probability of one action versus the rest) in the action it choses.</li>
  <li>Once we’ve obtained the value and policy losses from (3), our forward pass (propagation) through the network is complete. Now it’s time for the backward pass (propagation). Each worker uses these calculated losses to compute the gradients for its network parameters.</li>
  <li>We then use the gradients from (4) to update the global network parameters. This is when we reap the benefits of the asynchronous workers. The global network is constantly updated by each worker as they interact with its <em>own environment</em>. The intuition here is that because each worker has it’s own environment, the overall experience for training is more diverse.</li>
  <li>This concludes one round-trip (episode) of training. Then it repeats (1–5)</li>
</ol>

<p>Overall, the value estimates from the critic is used to update the policy in the actor, which works better than traditional policy gradient methods which doesn’t have a value etimate and solely tries to optimize the policy function. Therefore, it may be intuitive to let the critic learn faster (higher learning rate) than the actor.</p>

  </div><a class="u-url" href="/blog/reinforcement-learning/2019/02/27/a3c-rl-layments-explanation.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Henry&#39;s Blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Henry&#39;s Blog</li><li><a class="u-email" href="mailto:hanxiangp@gmail.com">hanxiangp@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/workofart"><svg class="svg-icon"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg> <span class="username">workofart</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>My learning notes, research, projects and musings.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
