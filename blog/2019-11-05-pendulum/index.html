<!DOCTYPE html>
<html lang="en">
<!-- Beautiful Jekyll 5.0.0 | Copyright Dean Attali 2020 -->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  

  

  <title>OpenAI Gym - Pendulum-v0</title>

  
  <meta name="author" content="Henry Pan">
  

  <meta name="description" content="Code Here 1. Goal The problem setting is to solve the Inverted Pendulum problem in OpenAI gym. Try to keep a frictionless pendulum standing up. 2. Environment The pendulum environment has a continuous state space as follows (copied from wiki): State Type: Box(3) Num State Observation Min Max 0 Angle1...">

  

  
  <meta name="keywords" content="career,software,engineering,life,education,inspiration,machine learning,deep learning,reinforcement learning,web development,react">
  

  

  

  

  
<!-- Google Analytics -->
<script>
  (function (i, s, o, g, r, a, m) {
    i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
      (i[r].q = i[r].q || []).push(arguments)
    }, i[r].l = 1 * new Date(); a = s.createElement(o),
      m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
  })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');
  ga('create', 'UA-103622213-1', 'auto');
  ga('send', 'pageview');
</script>
<!-- End Google Analytics -->


  


  
    
      
  <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">


    
      
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800">


    
  

  
    
      <link rel="stylesheet" href="/blog/assets/css/bootstrap-social.css">
    
      <link rel="stylesheet" href="/blog/assets/css/beautifuljekyll.css">
    
  

  

  
  
  

  

  
  <meta property="og:site_name" content="Henry's Blog">
  <meta property="og:title" content="OpenAI Gym - Pendulum-v0">
  <meta property="og:description" content="Code Here 1. Goal The problem setting is to solve the Inverted Pendulum problem in OpenAI gym. Try to keep a frictionless pendulum standing up. 2. Environment The pendulum environment has a continuous state space as follows (copied from wiki): State Type: Box(3) Num State Observation Min Max 0 Angle1...">

  

  
  <meta property="og:type" content="article">
  <meta property="og:article:author" content="Henry Pan">
  <meta property="og:article:published_time" content="2019-11-05T16:00:00-05:00">
  <meta property="og:url" content="/blog/2019-11-05-pendulum/">
  <link rel="canonical" href="/blog/2019-11-05-pendulum/">
  

  
  <meta name="twitter:card" content="summary">
  
  <meta name="twitter:site" content="@">
  <meta name="twitter:creator" content="@">

  <meta property="twitter:title" content="OpenAI Gym - Pendulum-v0">
  <meta property="twitter:description" content="Code Here 1. Goal The problem setting is to solve the Inverted Pendulum problem in OpenAI gym. Try to keep a frictionless pendulum standing up. 2. Environment The pendulum environment has a continuous state space as follows (copied from wiki): State Type: Box(3) Num State Observation Min Max 0 Angle1...">

  

  


  

  

</head>


<body>

  


  <nav class="navbar navbar-expand-xl navbar-light fixed-top navbar-custom top-nav-regular"><a class="navbar-brand" href="/blog">Henry's Blog</a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  <div class="collapse navbar-collapse" id="main-navbar">
    <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a class="nav-link" href="/blog/tags">Topics</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="https://www.henrypan.com">Author's home</a>
          </li>
        <li class="nav-item">
          <a class="nav-link" id="nav-search-link" href="#" title="Search">
            <span id="nav-search-icon" class="fa fa-search"></span>
            <span id="nav-search-text">Search</span>
          </a>
        </li></ul>
  </div>

  

  

</nav>



<div id="beautifuljekyll-search-overlay">

  <div id="nav-search-exit" title="Exit search">✕</div>
  <input type="text" id="nav-search-input" placeholder="Search">
  <ul id="search-results-container"></ul>
  
  <script src="https://unpkg.com/simple-jekyll-search@latest/dest/simple-jekyll-search.min.js"></script>
  <script>
    var searchjson = '[ \
       \
        { \
          "title"    : "ML by Hand", \
          "category" : "machine-learningresearchpythondeep-learningeducationframework", \
          "url"      : "/blog/2025-02-06-ml-by-hand/", \
          "date"     : "February  6, 2025" \
        }, \
       \
        { \
          "title"    : "Peaking into the real game", \
          "category" : "careerlife", \
          "url"      : "/blog/2021-11-15-peaking-into-the-real-game/", \
          "date"     : "November 15, 2021" \
        }, \
       \
        { \
          "title"    : "Tic-tac-toe Self-Play", \
          "category" : "reinforcement-learningresearch", \
          "url"      : "/blog/2019-12-06-tic-tac-toe-selfplay/", \
          "date"     : "December  6, 2019" \
        }, \
       \
        { \
          "title"    : "OpenAI Gym - Acrobot-v1", \
          "category" : "reinforcement-learningresearch", \
          "url"      : "/blog/2019-12-03-acrobot/", \
          "date"     : "December  3, 2019" \
        }, \
       \
        { \
          "title"    : "OpenAI Gym - Pendulum-v0", \
          "category" : "reinforcement-learningresearch", \
          "url"      : "/blog/2019-11-05-pendulum/", \
          "date"     : "November  5, 2019" \
        }, \
       \
        { \
          "title"    : "OpenAI Gym - MountainCar-v0", \
          "category" : "reinforcement-learningresearch", \
          "url"      : "/blog/2019-11-04-mountain-car/", \
          "date"     : "November  4, 2019" \
        }, \
       \
        { \
          "title"    : "BrawlStars AI Series (Part 2) - Reinforcement Learning", \
          "category" : "reinforcement-learningcomputer visiongameresearch", \
          "url"      : "/blog/2019-04-25-Brawlstars-RL/", \
          "date"     : "April 25, 2019" \
        }, \
       \
        { \
          "title"    : "BrawlStars AI Series (Part 1)", \
          "category" : "machine-learningdeep learninggameresearch", \
          "url"      : "/blog/2019-04-20-Brawlstars-AI/", \
          "date"     : "April 20, 2019" \
        }, \
       \
        { \
          "title"    : "Creating a Policy Gradient (PG) Agent to Trade", \
          "category" : "reinforcement-learningdeep learningtradingresearch", \
          "url"      : "/blog/2019-04-04-pg-trading/", \
          "date"     : "April  4, 2019" \
        }, \
       \
        { \
          "title"    : "Creating Neural Networks with Python/Keras/Tensorflow to Predict the Future", \
          "category" : "machine-learningtutorial", \
          "url"      : "/blog/2019-03-20-ml-tut-price-prediction/", \
          "date"     : "March 20, 2019" \
        }, \
       \
        { \
          "title"    : "Asynchronous Advantage Actor Critic (A3C)-Reinforcement Learning -Laymens Explanation", \
          "category" : "reinforcement-learningconcept", \
          "url"      : "/blog/2019-02-27-a3c-rl-layments-explanation/", \
          "date"     : "February 27, 2019" \
        }, \
       \
        { \
          "title"    : "React Redux Intro", \
          "category" : "reactfront-endsoftware development", \
          "url"      : "/blog/2019-01-26-react-redux-intro/", \
          "date"     : "January 26, 2019" \
        }, \
       \
        { \
          "title"    : "Career Paths in Data Science/Machine Learning", \
          "category" : "careermachine learningdata scienceanalyst", \
          "url"      : "/blog/2019-01-10-career-paths-in-ml/", \
          "date"     : "January 10, 2019" \
        }, \
       \
        { \
          "title"    : "Looking back, planning forward", \
          "category" : "careercomputer sciencebusiness", \
          "url"      : "/blog/2015-12-03-looking-back-planning-forward/", \
          "date"     : "December  3, 2015" \
        }, \
       \
        { \
          "title"    : "A few thoughts on choosing a career path", \
          "category" : "careeradvice", \
          "url"      : "/blog/2013-12-24-a-few-thoughts-on-choosing-a-career-path/", \
          "date"     : "December 24, 2013" \
        }, \
       \
        { \
          "title"    : "Different Life Experiences Bring Different Perspectives", \
          "category" : "life", \
          "url"      : "/blog/2013-11-16-different-life-experiences-bring-different-perspectives/", \
          "date"     : "November 16, 2013" \
        }, \
       \
        { \
          "title"    : "Finding the &#39;right&#39; route", \
          "category" : "educationbusinesslife", \
          "url"      : "/blog/2013-09-25-finding-the-right-route/", \
          "date"     : "September 25, 2013" \
        }, \
       \
        { \
          "title"    : "Step Back and Rethink", \
          "category" : "careercomputer sciencebusiness", \
          "url"      : "/blog/2013-08-07-step-back-and-rethink/", \
          "date"     : "August  7, 2013" \
        }, \
       \
        { \
          "title"    : "One Sentence Summary of Books", \
          "category" : "books", \
          "url"      : "/blog/2013-07-18-one-sentence-summary-books/", \
          "date"     : "July 18, 2013" \
        }, \
       \
        { \
          "title"    : "1st day at McKinsey", \
          "category" : "corporate culture", \
          "url"      : "/blog/2013-05-15-first-day-at-mckinsey/", \
          "date"     : "May 15, 2013" \
        }, \
       \
        { \
          "title"    : "Dinner Talk", \
          "category" : "inspirationcareer", \
          "url"      : "/blog/2013-03-30-dinner-talk/", \
          "date"     : "March 30, 2013" \
        }, \
       \
        { \
          "title"    : "Movie Review - &#39;Accepted&#39;", \
          "category" : "movieeducation", \
          "url"      : "/blog/2013-03-16-movie-review-accepted/", \
          "date"     : "March 16, 2013" \
        }, \
       \
        { \
          "title"    : "Some Inspirational People", \
          "category" : "inspirationcareer", \
          "url"      : "/blog/2013-03-02-some-inspirational-people/", \
          "date"     : "March  2, 2013" \
        }, \
       \
       \
        { \
          "title"    : "Henry&#39;s Blog", \
          "category" : "page", \
          "url"      : "/blog/", \
          "date"     : "January 1, 1970" \
        }, \
       \
        { \
          "title"    : "Tag Index", \
          "category" : "page", \
          "url"      : "/blog/tags/", \
          "date"     : "January 1, 1970" \
        }, \
       \
        { \
          "title"    : "Henry&#39;s Blog", \
          "category" : "page", \
          "url"      : "/blog/page2/", \
          "date"     : "January 1, 1970" \
        }, \
       \
        { \
          "title"    : "Henry&#39;s Blog", \
          "category" : "page", \
          "url"      : "/blog/page3/", \
          "date"     : "January 1, 1970" \
        }, \
       \
        { \
          "title"    : "Henry&#39;s Blog", \
          "category" : "page", \
          "url"      : "/blog/page4/", \
          "date"     : "January 1, 1970" \
        }, \
       \
        { \
          "title"    : "Henry&#39;s Blog", \
          "category" : "page", \
          "url"      : "/blog/page5/", \
          "date"     : "January 1, 1970" \
        } \
       \
    ]';
    searchjson = JSON.parse(searchjson);

    var sjs = SimpleJekyllSearch({
      searchInput: document.getElementById('nav-search-input'),
      resultsContainer: document.getElementById('search-results-container'),
      json: searchjson
    });
  </script>
</div>





  <!-- TODO this file has become a mess, refactor it -->







<header class="header-section ">

<div class="intro-header no-img">
  <div class="container-md">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
        <div class="post-heading">
          <h1>OpenAI Gym - Pendulum-v0</h1>
          

          
            <span class="post-meta">Posted on November 5, 2019</span>
            
            
          
        </div>
      </div>
    </div>
  </div>
</div>
</header>





<div class=" container-md ">
  <div class="row">
    <div class=" col-xl-8 offset-xl-2 col-lg-10 offset-lg-1 ">

      

      

      <article role="main" class="blog-post">
        <h2 id="code">Code</h2>

<p><a href="https://github.com/workofart/openai-gym-baselines/tree/master/Pendulum-v0">Here</a></p>

<p><br /></p>

<h2 id="1-goal">1. Goal</h2>
<p>The problem setting is to solve the <a href="https://gym.openai.com/envs/MountainCarContinuous-v0/">Inverted Pendulum</a> problem in OpenAI gym. Try to keep a frictionless pendulum standing up.</p>

<p><img src="https://github.com/workofart/openai-gym-baselines/raw/master/Pendulum-v0/test-run.gif" alt="test-run" style="zoom: 67%;" /></p>

<h2 id="2-environment">2. Environment</h2>
<p>The pendulum environment has a continuous state space as follows (copied from <a href="https://github.com/openai/gym/wiki/Pendulum-v0">wiki</a>):</p>

<p><br /></p>

<h4 id="state">State</h4>

<p>Type: Box(3)</p>

<table>
  <thead>
    <tr>
      <th>Num</th>
      <th>State</th>
      <th>Observation</th>
      <th>Min</th>
      <th>Max</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>Angle1</td>
      <td>cos(theta)</td>
      <td>-1.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <td>1</td>
      <td>Angle2</td>
      <td>sin(theta)</td>
      <td>-1.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <td>2</td>
      <td>Velocity</td>
      <td>theta dot</td>
      <td>-8.0</td>
      <td>8.0</td>
    </tr>
  </tbody>
</table>

<p><br /></p>

<h4 id="actions">Actions</h4>

<p>Type: Box(1)</p>

<table>
  <thead>
    <tr>
      <th>Num</th>
      <th>Action</th>
      <th>Min</th>
      <th>Max</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>Joint effort</td>
      <td>-2.0</td>
      <td>2.0</td>
    </tr>
  </tbody>
</table>

<p><br /></p>

<h4 id="reward">Reward</h4>

<p>The precise equation for reward:</p>

\[-(\theta^2 + 0.1*\theta_{dot}^2 + 0.001*\text{action}^2)\]

<p>Theta is normalized between -pi and pi. Therefore, the lowest reward is \(-(\pi^2 + 0.1*8^2 + 0.001*2^2) = -16.2736044\), and the highest reward is \(0\). In essence, the goal is to remain at zero angle (vertical), with the least rotational velocity, and the least effort.</p>

<p><br /></p>

<h4 id="starting-state">Starting State</h4>

<p>Random angle from \(-\pi\) to \(\pi\), and random velocity between <code class="language-plaintext highlighter-rouge">-1</code> and <code class="language-plaintext highlighter-rouge">1</code></p>

<p><br /></p>

<h4 id="episode-termination">Episode Termination</h4>

<p>There is no specified termination. Adding a maximum number of steps might be a good idea.</p>

<p>NOTE: Your environment object could be wrapped by the TimeLimit wrapper, if created using the <code class="language-plaintext highlighter-rouge">gym.make</code> method. In that case it will terminate after 200 steps.</p>

<p><br /></p>

<h4 id="solved-condition">Solved Condition</h4>

<p>There are no specific requirements, see the <strong>experiments section</strong> for a comparison of performance with the leaderboard.</p>

<p><br /></p>

<h2 id="3-approach">3. Approach</h2>
<p>The approach uses the one-step <em>actor-critic</em> (episodic) algorithm with <em>temporal difference</em> for estimating the advantage function for the critic. The policy will be a continuous one, namely the gaussian function.</p>

<p><br /></p>

<h3 id="31-discretization">3.1 Discretization</h3>

<p>Since the state space is continuous and is comprised of angle of the pendulum and the velocity we will discretize them separately using a radial basis function (RBF) as follows:</p>

<ul>
  <li>
    <p>Angle1 and Angle2 will <em>each</em> be discretized into 15 radial basis kernels</p>
  </li>
  <li>
    <p>Pendulum velocity will be discretized into 15 radial basis kernels.</p>
  </li>
  <li>
    <p>Essentially, each “transformed” state will be computed by measuring how far the raw state is from each of the 15 radial basis kernel centers.</p>
  </li>
</ul>

<p>Therefore, in total, there will be \(15^3=3375\) states.</p>

<p><br /></p>

<h3 id="32-exploration-vs-exploitation">3.2 Exploration vs Exploitation</h3>

<p>To overcome the exploration-exploitation dilemma, we will be slowly decreasing the standard deviation of the gaussian function. This will allow the agent’s action to vary a lot initially for exploration. At later stages of training, since the standard deviation is smaller, the actions will tend to be closer to the estimated gaussian mean, thus exploiting the learnt policy weights for a more accurate action selection.</p>

<p><br /></p>

<h3 id="33-gaussian-policy">3.3 Gaussian Policy</h3>

<p>To generate continuous actions for this problem, the gaussian policy is a simple and good baseline to start off with.</p>

<p>\(\Large
\pi(a \mid s, \theta) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(a - \mu(s, \theta))^2}{2\sigma^2}}\)
We can see that there are three variables that we need to provide for the policy to generate an action:
\(\mu = \text{The mean given the state and weights, this will be approximated by a function approximator}\\
\sigma = \text{The standard deviation parameter for this policy} \\
\theta = \text{The weights of the policy that will be trained}\)</p>

<p><br /></p>

<h3 id="34-linear-value-function">3.4 Linear Value Function</h3>

<p>Not to complicate our approach, our value function is defined to be a function approximator that has the same number of parameters as the number of states, which is 3375. The prediction of the value given the state is just a dot product between the current weights and the given state.</p>

<p><br /></p>

<h3 id="35-training">3.5 Training</h3>

<p>As the name suggest, the agent is trained one step at a time, which means the learning happens after every timestep (max 200 timestep make up one episode).</p>

<p><br /></p>

<p><strong>Policy (Actor) and Value (Critic) Function Weight Update</strong></p>

\[\Large
\begin{align*}
\alpha_v &amp;= \text{Value function learning rate}\\
\alpha_p &amp;= \text{Policy function learning rate}\\
\hat{v} &amp;= \text{Estimated value for the given state} \\
\theta_v &amp;= \text{Value function parameterization weights}\\
\theta_p &amp;= \text{Policy function parameterization weights}\\
\delta &amp;= \text{Advantage}\\
\gamma &amp;= \text{Discount rate}\\
I &amp;= \text{Policy parameter scaling factor, initially 1} \\
R &amp;= \text{Reward}\\ \\

\delta &amp;\leftarrow R + \gamma \hat{v}(S', \theta_v) - \hat{v}(S, \theta_v)\\
\theta_v &amp;\leftarrow \theta_v + \alpha_v \delta \triangledown \hat{v}(S, \theta_v) \\
\theta_p &amp;\leftarrow \theta_p + \alpha_p I \delta \triangledown \ln{\pi(A \mid S, \theta_p)} \\
I &amp;\leftarrow \gamma I \\
\end{align*}\]

<p><br /></p>

<h3 id="36-hyperparameters">3.6 Hyperparameters</h3>

<ul>
  <li>Discount rate (\(\gamma\)): 0.99</li>
  <li>Actor learning rate (\(\alpha_v\)): \(1e^{-4}\)</li>
  <li>Critic learning rate (\(\alpha_p\)): \(5e^{-3}\)</li>
  <li>Gaussian Policy standard deviation (\(\sigma_p\)): 0.5 decreasing to 0.1 linearly over all training episodes</li>
  <li>Radial Basis Function Kernel Width (\(\sigma_{rbf}\)): 0.1</li>
  <li>Total number of RBF kernels: 15 for each dimension of the state.</li>
</ul>

<p><br /></p>

<h2 id="4-experiment--findings">4. Experiment &amp; Findings</h2>

<table>
  <thead>
    <tr>
      <th># Episodes</th>
      <th>Training Time</th>
      <th>Avg Reward (last 100 episodes)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>2000</td>
      <td>213 seconds</td>
      <td>-146</td>
    </tr>
    <tr>
      <td>1000</td>
      <td>117 seconds</td>
      <td>-151</td>
    </tr>
    <tr>
      <td>500</td>
      <td>76 seconds</td>
      <td>-215</td>
    </tr>
    <tr>
      <td>200</td>
      <td>23.55 seconds</td>
      <td>-1062</td>
    </tr>
  </tbody>
</table>

<p><img src="https://github.com/workofart/openai-gym-baselines/raw/master/Pendulum-v0/500_ep.png" alt="500ep" style="zoom: 67%;" /></p>

<p><img src="https://github.com/workofart/openai-gym-baselines/raw/master/Pendulum-v0/2000_ep.png" alt="2000ep" style="zoom: 67%;" /></p>

<p>We can see that there is a significant improvement in performance after training for 1000 episodes. And by looking at the rewards graph, for the 2000 episode training instance, it has converged to &gt; -200 rewards at around 500 episodes. This means that perhaps a better initial and decay value for \(\sigma_p\) can be chosen to speed up the convergence rate in shorter training sessions.</p>

<p>It also shows that having 15 RBF kernels for each state dimension is enough for discretizing the state space. Since this hyperparameter directly affects the training speed and memory usage during training, it is often necessary to revisit this when it becomes a bottleneck.</p>

<p>Compared to the leaderboard below, our performance is within the range of the average users.</p>

<table>
  <thead>
    <tr>
      <th>User</th>
      <th>Best 100-episode performance</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><a href="https://github.com/msinto93">msinto93</a></td>
      <td>-123.11 ± 6.86</td>
    </tr>
    <tr>
      <td><a href="https://github.com/msinto93">msinto93</a></td>
      <td>-123.79 ± 6.90</td>
    </tr>
    <tr>
      <td><a href="https://github.com/heerad">heerad</a></td>
      <td>-134.48 ± 9.07</td>
    </tr>
    <tr>
      <td><a href="https://github.com/Bhaney44">BS Haney</a></td>
      <td>-135</td>
    </tr>
    <tr>
      <td><a href="https://github.com/ThyrixYang">ThyrixYang</a></td>
      <td>-136.16 ± 11.97</td>
    </tr>
    <tr>
      <td><a href="https://github.com/lirnli">lirnli</a></td>
      <td>-152.24 ± 10.87</td>
    </tr>
  </tbody>
</table>

<p><br /></p>

<h2 id="5-next-steps">5. Next Steps</h2>

<p>Fine tune the initial and decay value for \(\sigma_p\) and see its effect on the convergence rate and overall performance.</p>

<p>Remember that we’re still using a very simple linear function approximator for our value function. It works pretty well even with this simple baseline setup, which makes me wonder how much of an improvement will we have if we try something more powerful.</p>

<p>Try to shrink the state discretization to less RBF kernels, and see if there’s a significant reduction in training efficiency or effectiveness.</p>

<p>Overall, this was a very shallow dive into actor-critic, and there are far more intricacies to this method. I will try to explore other alternatives with the OpenAI gym. Stay tuned.</p>

      </article>

      
        <div class="blog-tags">
          <span>Tags:</span>
          
            <a href="/blog/tags#reinforcement-learning">reinforcement-learning</a>
          
            <a href="/blog/tags#research">research</a>
          
        </div>
      

      

      
        <!-- Check if any share-links are active -->





      

      <ul class="pagination blog-pager">
        
        <li class="page-item previous">
          <a class="page-link" href="/blog/2019-11-04-mountain-car/" data-toggle="tooltip" data-placement="top" title="OpenAI Gym - MountainCar-v0">&larr; Previous Post</a>
        </li>
        
        
        <li class="page-item next">
          <a class="page-link" href="/blog/2019-12-03-acrobot/" data-toggle="tooltip" data-placement="top" title="OpenAI Gym - Acrobot-v1">Next Post &rarr;</a>
        </li>
        
      </ul>
      
  
  
  

  


  



    </div>
  </div>
</div>


  <footer>
  <div class="container-md beautiful-jekyll-footer">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
      <ul class="list-inline text-center footer-links"><li class="list-inline-item">
    <a href="mailto:hanxiangp@gmail.com" title="Email me">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Email me</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://github.com/workofart" title="GitHub">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-github fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">GitHub</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://linkedin.com/in/pan-henry" title="LinkedIn">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-linkedin fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">LinkedIn</span>
   </a>
  </li></ul>

      
      <p class="copyright text-muted">
      
        Henry Pan
        &nbsp;&bull;&nbsp;
      
      2025

      

      
      </p>
      <p class="theme-by text-muted">
        Powered by
        <a href="https://beautifuljekyll.com">Beautiful Jekyll</a>
      </p>
      </div>
    </div>
  </div>
</footer>


  
  
    
  <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=" crossorigin="anonymous"></script>


  
    
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>


  
    
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>


  



  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
      <script src="/blog/assets/js/beautifuljekyll.js"></script>
    
  





  
    
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


  





</body>
</html>
