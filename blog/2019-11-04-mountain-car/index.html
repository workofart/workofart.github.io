<!DOCTYPE html>
<html lang="en">
<!-- Beautiful Jekyll 5.0.0 | Copyright Dean Attali 2020 -->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  

  

  <title>OpenAI Gym - MountainCar-v0</title>

  
  <meta name="author" content="Henry Pan">
  

  <meta name="description" content="Code Here 1. Goal The problem setting is to solve the Continuous MountainCar problem in OpenAI gym. 2. Environment The mountain car follows a continuous state space as follows(copied from wiki): The acceleration of the car is controlled via the application of a force which takes values in the range...">

  

  
  <meta name="keywords" content="career,software,engineering,life,education,inspiration,machine learning,deep learning,reinforcement learning,web development,react">
  

  

  

  

  
<!-- Google Analytics -->
<script>
  (function (i, s, o, g, r, a, m) {
    i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
      (i[r].q = i[r].q || []).push(arguments)
    }, i[r].l = 1 * new Date(); a = s.createElement(o),
      m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
  })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');
  ga('create', 'UA-103622213-1', 'auto');
  ga('send', 'pageview');
</script>
<!-- End Google Analytics -->


  


  
    
      
  <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">


    
      
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800">


    
  

  
    
      <link rel="stylesheet" href="/blog/assets/css/bootstrap-social.css">
    
      <link rel="stylesheet" href="/blog/assets/css/beautifuljekyll.css">
    
  

  

  
  
  

  

  
  <meta property="og:site_name" content="Henry's Blog">
  <meta property="og:title" content="OpenAI Gym - MountainCar-v0">
  <meta property="og:description" content="Code Here 1. Goal The problem setting is to solve the Continuous MountainCar problem in OpenAI gym. 2. Environment The mountain car follows a continuous state space as follows(copied from wiki): The acceleration of the car is controlled via the application of a force which takes values in the range...">

  

  
  <meta property="og:type" content="article">
  <meta property="og:article:author" content="Henry Pan">
  <meta property="og:article:published_time" content="2019-11-04T18:00:00-05:00">
  <meta property="og:url" content="/blog/2019-11-04-mountain-car/">
  <link rel="canonical" href="/blog/2019-11-04-mountain-car/">
  

  
  <meta name="twitter:card" content="summary">
  
  <meta name="twitter:site" content="@">
  <meta name="twitter:creator" content="@">

  <meta property="twitter:title" content="OpenAI Gym - MountainCar-v0">
  <meta property="twitter:description" content="Code Here 1. Goal The problem setting is to solve the Continuous MountainCar problem in OpenAI gym. 2. Environment The mountain car follows a continuous state space as follows(copied from wiki): The acceleration of the car is controlled via the application of a force which takes values in the range...">

  

  


  

  

</head>


<body>

  


  <nav class="navbar navbar-expand-xl navbar-light fixed-top navbar-custom top-nav-regular"><a class="navbar-brand" href="/blog">Henry's Blog</a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  <div class="collapse navbar-collapse" id="main-navbar">
    <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a class="nav-link" href="/blog/tags">Topics</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="https://www.henrypan.com">Author's home</a>
          </li>
        <li class="nav-item">
          <a class="nav-link" id="nav-search-link" href="#" title="Search">
            <span id="nav-search-icon" class="fa fa-search"></span>
            <span id="nav-search-text">Search</span>
          </a>
        </li></ul>
  </div>

  

  

</nav>



<div id="beautifuljekyll-search-overlay">

  <div id="nav-search-exit" title="Exit search">✕</div>
  <input type="text" id="nav-search-input" placeholder="Search">
  <ul id="search-results-container"></ul>
  
  <script src="https://unpkg.com/simple-jekyll-search@latest/dest/simple-jekyll-search.min.js"></script>
  <script>
    var searchjson = '[ \
       \
        { \
          "title"    : "Tic-tac-toe Self-Play", \
          "category" : "reinforcement-learning", \
          "url"      : "/blog/2019-12-06-tic-tac-toe-selfplay/", \
          "date"     : "December  6, 2019" \
        }, \
       \
        { \
          "title"    : "OpenAI Gym - Acrobot-v1", \
          "category" : "reinforcement-learning", \
          "url"      : "/blog/2019-12-03-acrobot/", \
          "date"     : "December  3, 2019" \
        }, \
       \
        { \
          "title"    : "OpenAI Gym - Pendulum-v0", \
          "category" : "reinforcement-learning", \
          "url"      : "/blog/2019-11-05-pendulum/", \
          "date"     : "November  5, 2019" \
        }, \
       \
        { \
          "title"    : "OpenAI Gym - MountainCar-v0", \
          "category" : "reinforcement-learning", \
          "url"      : "/blog/2019-11-04-mountain-car/", \
          "date"     : "November  4, 2019" \
        }, \
       \
        { \
          "title"    : "BrawlStars AI Series (Part 2) - Reinforcement Learning", \
          "category" : "reinforcement-learning", \
          "url"      : "/blog/2019-04-25-Brawlstars-RL/", \
          "date"     : "April 25, 2019" \
        }, \
       \
        { \
          "title"    : "BrawlStars AI Series (Part 1)", \
          "category" : "machine-learning", \
          "url"      : "/blog/2019-04-20-Brawlstars-AI/", \
          "date"     : "April 20, 2019" \
        }, \
       \
        { \
          "title"    : "Creating a Policy Gradient (PG) Agent to Trade", \
          "category" : "reinforcement-learning", \
          "url"      : "/blog/2019-04-04-pg-trading/", \
          "date"     : "April  4, 2019" \
        }, \
       \
        { \
          "title"    : "Creating Neural Networks with Python/Keras/Tensorflow to Predict the Future", \
          "category" : "machine-learning", \
          "url"      : "/blog/2019-03-20-ml-tut-price-prediction/", \
          "date"     : "March 20, 2019" \
        }, \
       \
        { \
          "title"    : "Asynchronous Advantage Actor Critic (A3C)-Reinforcement Learning -Laymens Explanation", \
          "category" : "reinforcement-learning", \
          "url"      : "/blog/2019-02-27-a3c-rl-layments-explanation/", \
          "date"     : "February 27, 2019" \
        }, \
       \
        { \
          "title"    : "React Redux Intro", \
          "category" : "react", \
          "url"      : "/blog/2019-01-26-react-redux-intro/", \
          "date"     : "January 26, 2019" \
        }, \
       \
        { \
          "title"    : "Career Paths in Data Science/Machine Learning", \
          "category" : "career", \
          "url"      : "/blog/2019-01-10-career-paths-in-ml/", \
          "date"     : "January 10, 2019" \
        }, \
       \
        { \
          "title"    : "Looking back, planning forward", \
          "category" : "career", \
          "url"      : "/blog/2015-12-03-looking-back-planning-forward/", \
          "date"     : "December  3, 2015" \
        }, \
       \
        { \
          "title"    : "A few thoughts on choosing a career path", \
          "category" : "career", \
          "url"      : "/blog/2013-12-24-a-few-thoughts-on-choosing-a-career-path/", \
          "date"     : "December 24, 2013" \
        }, \
       \
        { \
          "title"    : "Different Life Experiences Bring Different Perspectives", \
          "category" : "life", \
          "url"      : "/blog/2013-11-16-different-life-experiences-bring-different-perspectives/", \
          "date"     : "November 16, 2013" \
        }, \
       \
        { \
          "title"    : "Finding the &#39;right&#39; route", \
          "category" : "education", \
          "url"      : "/blog/2013-09-25-finding-the-right-route/", \
          "date"     : "September 25, 2013" \
        }, \
       \
        { \
          "title"    : "Step Back and Rethink", \
          "category" : "career", \
          "url"      : "/blog/2013-08-07-step-back-and-rethink/", \
          "date"     : "August  7, 2013" \
        }, \
       \
        { \
          "title"    : "One Sentence Summary of Books", \
          "category" : "books", \
          "url"      : "/blog/2013-07-18-one-sentence-summary-books/", \
          "date"     : "July 18, 2013" \
        }, \
       \
        { \
          "title"    : "1st day at McKinsey", \
          "category" : "culture", \
          "url"      : "/blog/2013-05-15-first-day-at-mckinsey/", \
          "date"     : "May 15, 2013" \
        }, \
       \
        { \
          "title"    : "Dinner Talk", \
          "category" : "inspirationcareer", \
          "url"      : "/blog/2013-03-30-dinner-talk/", \
          "date"     : "March 30, 2013" \
        }, \
       \
        { \
          "title"    : "Movie Review - &#39;Accepted&#39;", \
          "category" : "movieeducation", \
          "url"      : "/blog/2013-03-16-movie-review-accepted/", \
          "date"     : "March 16, 2013" \
        }, \
       \
        { \
          "title"    : "Some Inspirational People", \
          "category" : "inspirationcareer", \
          "url"      : "/blog/2013-03-02-some-inspirational-people/", \
          "date"     : "March  2, 2013" \
        }, \
       \
       \
        { \
          "title"    : "Henry&#39;s Blog", \
          "category" : "page", \
          "url"      : "/blog/", \
          "date"     : "January 1, 1970" \
        }, \
       \
        { \
          "title"    : "Tag Index", \
          "category" : "page", \
          "url"      : "/blog/tags/", \
          "date"     : "January 1, 1970" \
        }, \
       \
        { \
          "title"    : "Henry&#39;s Blog", \
          "category" : "page", \
          "url"      : "/blog/page2/", \
          "date"     : "January 1, 1970" \
        }, \
       \
        { \
          "title"    : "Henry&#39;s Blog", \
          "category" : "page", \
          "url"      : "/blog/page3/", \
          "date"     : "January 1, 1970" \
        }, \
       \
        { \
          "title"    : "Henry&#39;s Blog", \
          "category" : "page", \
          "url"      : "/blog/page4/", \
          "date"     : "January 1, 1970" \
        }, \
       \
        { \
          "title"    : "Henry&#39;s Blog", \
          "category" : "page", \
          "url"      : "/blog/page5/", \
          "date"     : "January 1, 1970" \
        } \
       \
    ]';
    searchjson = JSON.parse(searchjson);

    var sjs = SimpleJekyllSearch({
      searchInput: document.getElementById('nav-search-input'),
      resultsContainer: document.getElementById('search-results-container'),
      json: searchjson
    });
  </script>
</div>





  <!-- TODO this file has become a mess, refactor it -->







<header class="header-section ">

<div class="intro-header no-img">
  <div class="container-md">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
        <div class="post-heading">
          <h1>OpenAI Gym - MountainCar-v0</h1>
          

          
            <span class="post-meta">Posted on November 4, 2019</span>
            
            
          
        </div>
      </div>
    </div>
  </div>
</div>
</header>





<div class=" container-md ">
  <div class="row">
    <div class=" col-xl-8 offset-xl-2 col-lg-10 offset-lg-1 ">

      

      

      <article role="main" class="blog-post">
        <h2 id="code">Code</h2>

<p><a href="https://github.com/workofart/openai-gym-baselines/tree/master/MountainCarContinuous-v0">Here</a></p>

<h2 id="1-goal">1. Goal</h2>
<p>The problem setting is to solve the <a href="https://gym.openai.com/envs/MountainCarContinuous-v0/">Continuous MountainCar</a> problem in OpenAI gym.</p>

<p><img src="https://github.com/workofart/openai-gym-baselines/raw/master/MountainCarContinuous-v0/test-run.gif" alt="test-run" />
<br /></p>

<h2 id="2-environment">2. Environment</h2>

<p>The mountain car follows a continuous state space as follows(copied from <a href="https://github.com/openai/gym/wiki/MountainCarContinuous-v0">wiki</a>):</p>

<p>The acceleration of the car is controlled via the application of a force which takes values in the range [1, 1]. The states are the position of the car in the horizontal axis on the range [1.2, 0.6] and its velocity on the range [0.07, 0.07]. The goal is to get the car to accelerate up the hill and get to the flag.</p>

<p><br /></p>

<h4 id="state">State</h4>

<table>
  <thead>
    <tr>
      <th>Num</th>
      <th>Observation</th>
      <th>Min</th>
      <th>Max</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>Car Position</td>
      <td>-1.2</td>
      <td>0.6</td>
    </tr>
    <tr>
      <td>1</td>
      <td>Car Velocity</td>
      <td>-0.07</td>
      <td>0.07</td>
    </tr>
  </tbody>
</table>

<p>Note that velocity has been constrained to facilitate exploration, but this constraint might be relaxed in a more challenging version.</p>

<p><br /></p>

<h4 id="actions">Actions</h4>

<table>
  <thead>
    <tr>
      <th>Num</th>
      <th>Action</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>Push car to the left (negative value) or to the right (positive value)</td>
    </tr>
  </tbody>
</table>

<p><br /></p>

<h4 id="reward">Reward</h4>

<p>Reward is <code class="language-plaintext highlighter-rouge">100</code> for reaching the target of the hill on the right hand side, minus the squared sum of actions from start to goal.</p>

<p>This reward function raises an exploration challenge, because if the agent does not reach the target soon enough, it will figure out that it is better not to move, and won’t find the target anymore.</p>

<p>Note that this reward is unusual with respect to most published work, where the goal was to reach the target as fast as possible, hence favouring a bang-bang strategy.</p>

<p><br /></p>

<h4 id="starting-state">Starting State</h4>

<p>Position between <code class="language-plaintext highlighter-rouge">-0.6</code> and <code class="language-plaintext highlighter-rouge">-0.4</code>, null velocity.</p>

<p><br /></p>

<h4 id="episode-termination">Episode Termination</h4>

<p>Position equal to <code class="language-plaintext highlighter-rouge">0.5</code>. A constraint on velocity might be added in a more challenging version.</p>

<p>The episode will terminate either when the car has reached the goal OR when the total number of time steps reached 1000 regardless of reaching the goal or not.</p>

<p><br /></p>

<h4 id="solved-requirements">Solved Requirements</h4>

<p>Get a reward over <code class="language-plaintext highlighter-rouge">90</code>. This value might be tuned.</p>

<p><br /></p>

<h2 id="3-approach">3. Approach</h2>

<p>The approach uses the <em>policy gradient</em> algorithm with a baseline to reduce variance. Even though the state space is continuous, in this attempt, we will be using a discrete softmax policy. In other words, the continuous state space will be discretized into buckets of states that will be fed to the agent that will output a discrete action either <code class="language-plaintext highlighter-rouge">[-1, 0, 1]</code>, which is <code class="language-plaintext highlighter-rouge">[left, no-action, right]</code>.</p>

<p><br /></p>

<h3 id="31-discretization">3.1 Discretization</h3>

<p>Since there are two dimensions in the state space, namely position and velocity. We will discretize them separately into <strong>150</strong> buckets and <strong>120</strong> buckets for position and velocity respectively.</p>

<p><br /></p>

<h3 id="32-exploration-vs-exploitation">3.2 Exploration vs Exploitation</h3>

<p>To overcome the exploration-exploitation dilemma, we will be using the epsilon-greedy approach to slowly decrease the randomization factor overtime. This will ensure that our agent will have a wide variety of state-action training samples and in the later part of the training, it will allow the agent to follow it’s own “trained strategy” as opposed to random actions.</p>

<p>Technically, in the code, we will be using a <strong>temperature</strong> term to smooth the probability of actions, and <strong>epsilon</strong> to decide between whether to take a random action or the predicted action output from the policy.</p>

<p><br /></p>

<h3 id="33-training">3.3 Training</h3>

<p><strong>Monte Carlo</strong></p>

<p>The training process follows a <em>Monte Carlo</em> method. This means the training only takes place after an entire episode is completed, and replays the accumulated state/action/reward/next state for training. This is at one end of the spectrum, the other end of the spectrum is called <em>1-Step Temporal Difference</em> learning. So Monte Carlo is essentially a <em>\(\infty\)-step Temporal Difference</em> learning. There is a balance between when you want to train the agent. Training it too early could render very messy results and thus might be harder to converge. Training it too late might prolong the training duration for convergence. <strong>The criteria for choosing the method depends heavily on the problem itself.</strong></p>

<p><img src="/blog/assets/images/rl/td_vs_montecarlo.png" width="800" /></p>

<center>[1] Richard S. Sutton and Andrew G. Barto. 2018. *Reinforcement Learning: An Introduction*. A Bradford Book, Cambridge, MA, USA.</center>

<p><br /></p>

<p><strong>Policy Gradient Weight Update</strong>
\(\alpha_v = \text{Value function learning rate}\\
\alpha_p = \text{Policy function learning rate}\\
\hat{v} = \text{Estimated value for the given state} \\
\theta_v = \text{Value function parameterization weights}\\
\theta_p = \text{Policy function parameterization weights}\\
\delta = \text{Advantage}\\
\gamma = \text{Discount rate}\\\
G = \text{Discounted Rewards}\)</p>

\[\Large
\delta \leftarrow G - \hat{v}(S, \theta_{v})\\
\Large
\theta_{v} \leftarrow \theta_{v} + \alpha_v \delta \triangledown \hat{v}(S, \theta_{v}) \\
\Large
\theta_{p} \leftarrow \theta_{p} + \alpha_p \delta \gamma \triangledown \ln{\pi(A \mid S, \theta_p)}\\\]

<p><strong>Gradient Calculation for Softmax Policy</strong>
\(a = \text{action}\\
a' = \text{selected action by policy}\\
w = \text{policy weights}\\
\tau = \text{temperature for epsilon-greedy}\\
s = \text{current state} \\
\pi(a \mid s) = \text{policy outputs an action given the current state} \\
\Large
\quad \quad \quad = \frac{e^{w_{s,a}}}{\sum_{a'}e^{w_{s,a'}}}\)</p>

\[\Large
\begin{align*}
    \triangledown_w \log \pi(a \mid s) &amp;= \frac{\partial \log \frac{e^{w_{s,a}} / \tau}{\sum_{a'}e^{w_{s,a'}} / \tau}}{\partial w} \\
    \newline \\
    \text{(Take logs)} \; &amp;= \frac{\partial \log \big(e^{w_{s,a}} / \tau\big) - \partial \log \big (\sum_{a'}e^{w_{s,a'}} / \tau \big)} {\partial w}\\
    \newline \\
    \text{(Chain &amp; Log rule)} &amp;= \frac{e^{w_{s, a}} / \tau  w_{s,a} / \tau}{e^{w_{s,a}} / \tau \ln{e}} - \frac{\sum_{a'} w_{s,a'} / \tau \; e^{w_{s,a'}/ \tau}}{\sum_{a'}e^{w_{s,a'}}/ \tau}\\
    \newline \\
    \text{(Simplify first equation)} \; &amp;= w_{s,a} / \tau - \frac{\sum_{a'} w_{s,a'} / \tau \; e^{w_{s,a'}/ \tau}}{\sum_{a'}e^{w_{s,a'}}/ \tau}\\
    \newline \\
    % \text{(Second equation substitute } \pi(a \mid s) \text{)} \; &amp;= w_{s,a} / \tau - \sum_{a'} w_{s,a'} / \tau \; \pi(a' \mid s)\\
    \text{(If Action = $a^\prime$) } &amp;= (1 - \frac{e^{w_{s, a}}}{\sum_{a'}e^{w_{s,a'}}}) / \tau \\
    &amp;= \frac{1 - \pi(a \mid s)}{\tau} \\
    \text{(If Action  $\neq a^\prime$) } &amp;= -(\frac{e^{w_{s, a}}}{\sum_{a'}e^{w_{s,a'}}}) / \tau \\
    &amp;= - \frac{\pi(a \mid s)}{\tau} \\
\end{align*}\]

<h3 id="34-hyperparameters">3.4 Hyperparameters</h3>

<ul>
  <li>Discount rate (\(\gamma\)): 0.999</li>
  <li>Value function learning rate (\(\alpha_v\)): \(1e^{-2}\)</li>
  <li>Policy function learning rate (\(\alpha_p\)): \(1e^{-3}\)</li>
  <li>Temperature (\(\tau\)): 1 decreasing to 0.5 linearly over all training episodes</li>
  <li>Discretization for position state: 150 buckets</li>
  <li>Discretization for velocity state: 120 buckets</li>
  <li>Epsilon-greedy (\(\epsilon\)): 1 decreasing to 0.1 linearly over all training episodes</li>
</ul>

<p><br /></p>

<h2 id="4-experiment--findings">4. Experiment &amp; Findings</h2>

<table>
  <thead>
    <tr>
      <th># Episodes</th>
      <th>Training Time</th>
      <th>Min. Reward</th>
      <th>Max Reward</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>2500</td>
      <td>473 seconds</td>
      <td>31.7</td>
      <td>90.6</td>
    </tr>
    <tr>
      <td>1500</td>
      <td>327 seconds</td>
      <td>31.5</td>
      <td>89.9</td>
    </tr>
    <tr>
      <td>1000</td>
      <td>254 seconds</td>
      <td>31.2</td>
      <td>89.3</td>
    </tr>
    <tr>
      <td>500</td>
      <td>154 seconds</td>
      <td>30.2</td>
      <td>86.7</td>
    </tr>
    <tr>
      <td>200</td>
      <td>77 seconds</td>
      <td>31.1</td>
      <td>84.6</td>
    </tr>
  </tbody>
</table>

<p><br /></p>

<h3 id="41-introducing-baseline-to-reduce-variance">4.1 Introducing baseline to reduce variance</h3>

<center><b>Without Baseline</b></center>

<p><img src="https://github.com/workofart/openai-gym-baselines/raw/master/MountainCarContinuous-v0/without_baseline.png" alt="without_baseline" style="zoom: 67%;" /></p>

<center>Training took: 280.78 seconds</center>

<hr />

<center><b>With Baseline</b></center>

<p><img src="https://github.com/workofart/openai-gym-baselines/raw/master/MountainCarContinuous-v0/with_baseline.png" alt="with_baseline" style="zoom:67%;" /></p>

<center>Training took: 253.72 seconds</center>

<p>The above comparison experiment is to show the effects of having a baseline on the agent’s performance. We can easily see from the rewards graph, the agent <strong>with baseline</strong> has a smaller variance in the rewards. This translates to a faster convergence rate.</p>

<p><br /></p>

<h3 id="42-discrete-vs-continuous-actions">4.2 Discrete vs Continuous Actions</h3>

<p>This problem (MountainCarContinuous-v0) was intended to be solved using a continuous action policy. However, I didn’t use continuous actions because I wanted to see how well a discrete-action agent could perform on this simple task. In conclusion, the number of episodes until convergence is quite good. The overall max reward reaches the goal of 90 at around ~1000 episodes.</p>

<p><br /></p>

<h3 id="43-performance">4.3 Performance</h3>

<p>Let’s look at the leaderboard of this problem and <strong>MountainCar-v0</strong>, which is a discrete version of the problem.</p>

<table>
  <thead>
    <tr>
      <th>MountainCar-v0</th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>User</strong></td>
      <td><strong>Episodes before solve</strong></td>
    </tr>
    <tr>
      <td><a href="https://github.com/ZhiqingXiao">Zhiqing Xiao</a></td>
      <td>0 (use close-form preset policy)</td>
    </tr>
    <tr>
      <td><a href="https://github.com/StepNeverStop">Keavnn</a></td>
      <td>47</td>
    </tr>
    <tr>
      <td><a href="https://github.com/ZhiqingXiao">Zhiqing Xiao</a></td>
      <td>75</td>
    </tr>
    <tr>
      <td><a href="https://github.com/roboticist-by-day">Mohith Sakthivel</a></td>
      <td>90</td>
    </tr>
    <tr>
      <td><a href="https://github.com/amohamed11">Anas Mohamed</a></td>
      <td>341</td>
    </tr>
    <tr>
      <td><a href="https://github.com/harshitandro">Harshit Singh Lodha</a></td>
      <td>643</td>
    </tr>
    <tr>
      <td><a href="https://github.com/CM-Data">Colin M</a></td>
      <td>944</td>
    </tr>
    <tr>
      <td><a href="https://github.com/jing582">jing582</a></td>
      <td>1119</td>
    </tr>
    <tr>
      <td><a href="https://github.com/DaveLeongSingapore">DaveLeongSingapore</a></td>
      <td>1967</td>
    </tr>
    <tr>
      <td><a href="https://github.com/Pechckin">Pechckin</a></td>
      <td>30</td>
    </tr>
    <tr>
      <td><a href="https://github.com/amitkvikram">Amit</a></td>
      <td>1000-1200</td>
    </tr>
    <tr>
      <td><a href="https://github.com/elcrion/mountain_car">Gleb I</a></td>
      <td>100</td>
    </tr>
  </tbody>
</table>

<table>
  <thead>
    <tr>
      <th>MountainCarContinuous-v0</th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>User</td>
      <td>Episodes before solve</td>
    </tr>
    <tr>
      <td><a href="https://github.com/Ashioto">Ashioto</a></td>
      <td>1</td>
    </tr>
    <tr>
      <td><a href="https://github.com/StepNeverStop">Keavnn</a></td>
      <td>11</td>
    </tr>
    <tr>
      <td><a href="https://github.com/camigord">camigord</a></td>
      <td>18</td>
    </tr>
    <tr>
      <td><a href="https://github.com/tobiassteidle">Tobias Steidle</a></td>
      <td>32</td>
    </tr>
    <tr>
      <td><a href="https://github.com/lirnli">lirnli</a></td>
      <td>33</td>
    </tr>
    <tr>
      <td><a href="https://github.com/Khev/RL-practice-keras/blob/master/DDPG/writeup_for_openai.ipynb">khev</a></td>
      <td>130</td>
    </tr>
    <tr>
      <td><a href="https://github.com/sanketsans/openAIenv/tree/master/CEM/mountainCar_Cont">Sanket Thakur</a></td>
      <td>140</td>
    </tr>
    <tr>
      <td><a href="https://github.com/Pechckin">Pechckin</a></td>
      <td>1</td>
    </tr>
    <tr>
      <td><a href="https://github.com/nikhilbarhate99">Nikhil Barhate</a></td>
      <td>200 (HAC)</td>
    </tr>
  </tbody>
</table>

<p>This shows that our discrete policy is a good baseline to start with before we dive into continuous actions. With continuous actions, debugging might be slightly harder as the policy function will be slightly more complicated, perhaps using a neural network approximating the policy function.</p>

<p><br /></p>

<h2 id="5-next-steps">5. Next Steps</h2>

<p>An obvious next step would be to try out continuous actions on this same problem and see how much more “efficient” and “effective” training can be. Intuitively, with continuous actions, you would expect a more fine-grain control of the car, and thus less “bouncing” around. Think driving with full petal and full brake, it’s not a very effective way to drive, is it?</p>

<p>Another thing that could be interesting to try out is to change the “n” in the <em>n-step Temporal Difference (TD)</em> learning and see the effects with regards to performance, convergence rate, reward variance. Right now we are using <em>Monte Carlo</em>, which is essentially a \(\infty\)-step TD method.</p>

<p>The current approach uses policy gradient as the approach to train the agent. There are some limitations in policy-based methods. There are other reinforcement learning algorithms that can be used to tackle this problem such as Deep Q-Learning, and Actor-Critic. I will be tackling another OpenAI Gym problem (<a href="https://github.com/openai/gym/wiki/Pendulum-v0">Pendulum-v0</a>) using actor-critic. Stay tuned for my next post.</p>


      </article>

      
        <div class="blog-tags">
          <span>Tags:</span>
          
            <a href="/blog/tags#reinforcement-learning">reinforcement-learning</a>
          
        </div>
      

      

      
        <!-- Check if any share-links are active -->





      

      <ul class="pagination blog-pager">
        
        <li class="page-item previous">
          <a class="page-link" href="/blog/2019-04-25-Brawlstars-RL/" data-toggle="tooltip" data-placement="top" title="BrawlStars AI Series (Part 2) - Reinforcement Learning">&larr; Previous Post</a>
        </li>
        
        
        <li class="page-item next">
          <a class="page-link" href="/blog/2019-11-05-pendulum/" data-toggle="tooltip" data-placement="top" title="OpenAI Gym - Pendulum-v0">Next Post &rarr;</a>
        </li>
        
      </ul>
      
  
  
  

  


  



    </div>
  </div>
</div>


  <footer>
  <div class="container-md beautiful-jekyll-footer">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
      <ul class="list-inline text-center footer-links"><li class="list-inline-item">
    <a href="mailto:hanxiangp@gmail.com" title="Email me">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Email me</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://github.com/workofart" title="GitHub">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-github fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">GitHub</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://linkedin.com/in/pan-henry" title="LinkedIn">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-linkedin fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">LinkedIn</span>
   </a>
  </li></ul>

      
      <p class="copyright text-muted">
      
        Henry Pan
        &nbsp;&bull;&nbsp;
      
      2021

      

      
      </p>
      <p class="theme-by text-muted">
        Powered by
        <a href="https://beautifuljekyll.com">Beautiful Jekyll</a>
      </p>
      </div>
    </div>
  </div>
</footer>


  
  
    
  <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=" crossorigin="anonymous"></script>


  
    
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>


  
    
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>


  



  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
      <script src="/blog/assets/js/beautifuljekyll.js"></script>
    
  





  
    
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


  





</body>
</html>
