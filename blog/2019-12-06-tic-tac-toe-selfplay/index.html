<!DOCTYPE html>
<html lang="en">
<!-- Beautiful Jekyll 5.0.0 | Copyright Dean Attali 2020 -->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  

  

  <title>Tic-tac-toe Self-Play</title>

  
  <meta name="author" content="Henry Pan">
  

  <meta name="description" content="Code Here 1. Goal The motivation of this project is AlphaGo and its mechanism of self-play. So I took on the task of implementing a very simple self-play mechanism that is able to train 2 agents in mastering tic-tac-toe with only the visible state/reward information. No information on the rules...">

  

  
  <meta name="keywords" content="career,software,engineering,life,education,inspiration,machine learning,deep learning,reinforcement learning,web development,react">
  

  

  

  

  
<!-- Google Analytics -->
<script>
  (function (i, s, o, g, r, a, m) {
    i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
      (i[r].q = i[r].q || []).push(arguments)
    }, i[r].l = 1 * new Date(); a = s.createElement(o),
      m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
  })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');
  ga('create', 'UA-103622213-1', 'auto');
  ga('send', 'pageview');
</script>
<!-- End Google Analytics -->


  


  
    
      
  <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">


    
      
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic">


    
      
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800">


    
  

  
    
      <link rel="stylesheet" href="/blog/assets/css/bootstrap-social.css">
    
      <link rel="stylesheet" href="/blog/assets/css/beautifuljekyll.css">
    
  

  

  
  
  

  

  
  <meta property="og:site_name" content="Henry's Blog">
  <meta property="og:title" content="Tic-tac-toe Self-Play">
  <meta property="og:description" content="Code Here 1. Goal The motivation of this project is AlphaGo and its mechanism of self-play. So I took on the task of implementing a very simple self-play mechanism that is able to train 2 agents in mastering tic-tac-toe with only the visible state/reward information. No information on the rules...">

  

  
  <meta property="og:type" content="article">
  <meta property="og:article:author" content="Henry Pan">
  <meta property="og:article:published_time" content="2019-12-06T16:00:00-05:00">
  <meta property="og:url" content="/blog/2019-12-06-tic-tac-toe-selfplay/">
  <link rel="canonical" href="/blog/2019-12-06-tic-tac-toe-selfplay/">
  

  
  <meta name="twitter:card" content="summary">
  
  <meta name="twitter:site" content="@">
  <meta name="twitter:creator" content="@">

  <meta property="twitter:title" content="Tic-tac-toe Self-Play">
  <meta property="twitter:description" content="Code Here 1. Goal The motivation of this project is AlphaGo and its mechanism of self-play. So I took on the task of implementing a very simple self-play mechanism that is able to train 2 agents in mastering tic-tac-toe with only the visible state/reward information. No information on the rules...">

  

  


  

  

</head>


<body>

  


  <nav class="navbar navbar-expand-xl navbar-light fixed-top navbar-custom top-nav-regular"><a class="navbar-brand" href="/blog">Henry's Blog</a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  <div class="collapse navbar-collapse" id="main-navbar">
    <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a class="nav-link" href="/blog/tags">Topics</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="https://www.henrypan.com">Author's home</a>
          </li>
        <li class="nav-item">
          <a class="nav-link" id="nav-search-link" href="#" title="Search">
            <span id="nav-search-icon" class="fa fa-search"></span>
            <span id="nav-search-text">Search</span>
          </a>
        </li></ul>
  </div>

  

  

</nav>



<div id="beautifuljekyll-search-overlay">

  <div id="nav-search-exit" title="Exit search">✕</div>
  <input type="text" id="nav-search-input" placeholder="Search">
  <ul id="search-results-container"></ul>
  
  <script src="https://unpkg.com/simple-jekyll-search@latest/dest/simple-jekyll-search.min.js"></script>
  <script>
    var searchjson = '[ \
       \
        { \
          "title"    : "Peaking into the real game", \
          "category" : "careerlife", \
          "url"      : "/blog/2021-11-15-peaking-into-the-real-game/", \
          "date"     : "November 15, 2021" \
        }, \
       \
        { \
          "title"    : "Tic-tac-toe Self-Play", \
          "category" : "reinforcement-learningresearch", \
          "url"      : "/blog/2019-12-06-tic-tac-toe-selfplay/", \
          "date"     : "December  6, 2019" \
        }, \
       \
        { \
          "title"    : "OpenAI Gym - Acrobot-v1", \
          "category" : "reinforcement-learningresearch", \
          "url"      : "/blog/2019-12-03-acrobot/", \
          "date"     : "December  3, 2019" \
        }, \
       \
        { \
          "title"    : "OpenAI Gym - Pendulum-v0", \
          "category" : "reinforcement-learningresearch", \
          "url"      : "/blog/2019-11-05-pendulum/", \
          "date"     : "November  5, 2019" \
        }, \
       \
        { \
          "title"    : "OpenAI Gym - MountainCar-v0", \
          "category" : "reinforcement-learningresearch", \
          "url"      : "/blog/2019-11-04-mountain-car/", \
          "date"     : "November  4, 2019" \
        }, \
       \
        { \
          "title"    : "BrawlStars AI Series (Part 2) - Reinforcement Learning", \
          "category" : "reinforcement-learningcomputer visiongameresearch", \
          "url"      : "/blog/2019-04-25-Brawlstars-RL/", \
          "date"     : "April 25, 2019" \
        }, \
       \
        { \
          "title"    : "BrawlStars AI Series (Part 1)", \
          "category" : "machine-learningdeep learninggameresearch", \
          "url"      : "/blog/2019-04-20-Brawlstars-AI/", \
          "date"     : "April 20, 2019" \
        }, \
       \
        { \
          "title"    : "Creating a Policy Gradient (PG) Agent to Trade", \
          "category" : "reinforcement-learningdeep learningtradingresearch", \
          "url"      : "/blog/2019-04-04-pg-trading/", \
          "date"     : "April  4, 2019" \
        }, \
       \
        { \
          "title"    : "Creating Neural Networks with Python/Keras/Tensorflow to Predict the Future", \
          "category" : "machine-learningtutorial", \
          "url"      : "/blog/2019-03-20-ml-tut-price-prediction/", \
          "date"     : "March 20, 2019" \
        }, \
       \
        { \
          "title"    : "Asynchronous Advantage Actor Critic (A3C)-Reinforcement Learning -Laymens Explanation", \
          "category" : "reinforcement-learningconcept", \
          "url"      : "/blog/2019-02-27-a3c-rl-layments-explanation/", \
          "date"     : "February 27, 2019" \
        }, \
       \
        { \
          "title"    : "React Redux Intro", \
          "category" : "reactfront-endsoftware development", \
          "url"      : "/blog/2019-01-26-react-redux-intro/", \
          "date"     : "January 26, 2019" \
        }, \
       \
        { \
          "title"    : "Career Paths in Data Science/Machine Learning", \
          "category" : "careermachine learningdata scienceanalyst", \
          "url"      : "/blog/2019-01-10-career-paths-in-ml/", \
          "date"     : "January 10, 2019" \
        }, \
       \
        { \
          "title"    : "Looking back, planning forward", \
          "category" : "careercomputer sciencebusiness", \
          "url"      : "/blog/2015-12-03-looking-back-planning-forward/", \
          "date"     : "December  3, 2015" \
        }, \
       \
        { \
          "title"    : "A few thoughts on choosing a career path", \
          "category" : "careeradvice", \
          "url"      : "/blog/2013-12-24-a-few-thoughts-on-choosing-a-career-path/", \
          "date"     : "December 24, 2013" \
        }, \
       \
        { \
          "title"    : "Different Life Experiences Bring Different Perspectives", \
          "category" : "life", \
          "url"      : "/blog/2013-11-16-different-life-experiences-bring-different-perspectives/", \
          "date"     : "November 16, 2013" \
        }, \
       \
        { \
          "title"    : "Finding the &#39;right&#39; route", \
          "category" : "educationbusinesslife", \
          "url"      : "/blog/2013-09-25-finding-the-right-route/", \
          "date"     : "September 25, 2013" \
        }, \
       \
        { \
          "title"    : "Step Back and Rethink", \
          "category" : "careercomputer sciencebusiness", \
          "url"      : "/blog/2013-08-07-step-back-and-rethink/", \
          "date"     : "August  7, 2013" \
        }, \
       \
        { \
          "title"    : "One Sentence Summary of Books", \
          "category" : "books", \
          "url"      : "/blog/2013-07-18-one-sentence-summary-books/", \
          "date"     : "July 18, 2013" \
        }, \
       \
        { \
          "title"    : "1st day at McKinsey", \
          "category" : "corporate culture", \
          "url"      : "/blog/2013-05-15-first-day-at-mckinsey/", \
          "date"     : "May 15, 2013" \
        }, \
       \
        { \
          "title"    : "Dinner Talk", \
          "category" : "inspirationcareer", \
          "url"      : "/blog/2013-03-30-dinner-talk/", \
          "date"     : "March 30, 2013" \
        }, \
       \
        { \
          "title"    : "Movie Review - &#39;Accepted&#39;", \
          "category" : "movieeducation", \
          "url"      : "/blog/2013-03-16-movie-review-accepted/", \
          "date"     : "March 16, 2013" \
        }, \
       \
        { \
          "title"    : "Some Inspirational People", \
          "category" : "inspirationcareer", \
          "url"      : "/blog/2013-03-02-some-inspirational-people/", \
          "date"     : "March  2, 2013" \
        }, \
       \
       \
        { \
          "title"    : "Henry&#39;s Blog", \
          "category" : "page", \
          "url"      : "/blog/", \
          "date"     : "January 1, 1970" \
        }, \
       \
        { \
          "title"    : "Tag Index", \
          "category" : "page", \
          "url"      : "/blog/tags/", \
          "date"     : "January 1, 1970" \
        }, \
       \
        { \
          "title"    : "Henry&#39;s Blog", \
          "category" : "page", \
          "url"      : "/blog/page2/", \
          "date"     : "January 1, 1970" \
        }, \
       \
        { \
          "title"    : "Henry&#39;s Blog", \
          "category" : "page", \
          "url"      : "/blog/page3/", \
          "date"     : "January 1, 1970" \
        }, \
       \
        { \
          "title"    : "Henry&#39;s Blog", \
          "category" : "page", \
          "url"      : "/blog/page4/", \
          "date"     : "January 1, 1970" \
        }, \
       \
        { \
          "title"    : "Henry&#39;s Blog", \
          "category" : "page", \
          "url"      : "/blog/page5/", \
          "date"     : "January 1, 1970" \
        } \
       \
    ]';
    searchjson = JSON.parse(searchjson);

    var sjs = SimpleJekyllSearch({
      searchInput: document.getElementById('nav-search-input'),
      resultsContainer: document.getElementById('search-results-container'),
      json: searchjson
    });
  </script>
</div>





  <!-- TODO this file has become a mess, refactor it -->







<header class="header-section ">

<div class="intro-header no-img">
  <div class="container-md">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
        <div class="post-heading">
          <h1>Tic-tac-toe Self-Play</h1>
          

          
            <span class="post-meta">Posted on December 6, 2019</span>
            
            
          
        </div>
      </div>
    </div>
  </div>
</div>
</header>





<div class=" container-md ">
  <div class="row">
    <div class=" col-xl-8 offset-xl-2 col-lg-10 offset-lg-1 ">

      

      

      <article role="main" class="blog-post">
        <h2 id="code">Code</h2>

<p><a href="https://github.com/workofart/selfplay-tictactoe">Here</a></p>

<h2 id="1-goal">1. Goal</h2>

<p>The motivation of this project is <a href="https://deepmind.com/research/case-studies/alphago-the-story-so-far">AlphaGo</a> and its mechanism of self-play. So I took on the task of implementing a very simple self-play mechanism that is able to train 2 agents in mastering tic-tac-toe with only the visible state/reward information. No information on the rules of the game or prior knowledge.</p>

<p>The goal of this project is two folds:</p>

<ol>
  <li>Train a reinforcement learning agent to beat the predefined min-max agent, this is a good starting point for self-play because it would be very inefficient to train two dummy agents to learn from scratch.</li>
  <li>Using the trained agent from step (1), self-play against a copy of itself. Obviously one will go first. We expect that given the mechanics of tic-tac-toe, the two agents will converge to a draw game every single time.</li>
</ol>

<p><br /></p>

<h2 id="2-environment">2. Environment</h2>

<p>The environment contains a <a href="https://en.wikipedia.org/wiki/Minimax">mini-max</a> tic-tac-toe player. This is used as a starting point for training the reinforcement learning agent.</p>

<p><br /></p>

<h3 id="state">State</h3>

<p>The states are stored in a 3x3 grid.</p>

<p><img src="/blog/assets/images/rl/tictactoe_board.png" alt="tictactoe_board" /></p>

<p><br /></p>

<h3 id="action">Action</h3>

<p>‘X’ - denotes the first player</p>

<p>‘O’ - denotes the second player</p>

<p><br /></p>

<h3 id="reward">Reward</h3>

<p>This wasn’t part of the original environment. This was intentionally left as a open-ended discussion which will be covered in section <strong>[3.1 Reward Design]</strong> since this will determine how well the agent can learn.</p>

<p><br /></p>

<h3 id="termination-condition">Termination Condition</h3>

<p>The game (episode) is terminated when either player wins or the game is drawed by having all the board cells filled with actions.</p>

<p><br /></p>

<h2 id="3-approach">3. Approach</h2>

<p><br /></p>

<h3 id="31-reward-function-design">3.1 Reward Function Design</h3>

<p><strong>Assumption:</strong> This reward design is assuming the agent always goes first in the game. This will change in the self-play setting, which is covered in <strong>[4.2 Optimal Policy Discussion]</strong></p>

<p><br /></p>

<p>The reward function is designed as follows:</p>

<ul>
  <li>if agent wins: +20</li>
  <li>if agent loses: 0</li>
  <li>if agent ties: +10</li>
  <li>For every step the agent makes: -2</li>
</ul>

<p><br /></p>

<p>The design of the rewards is closely related to <strong>how the game can terminate</strong>, specifically:</p>

<ul>
  <li>Tie when the board is fully occupied (9 pieces)</li>
  <li>Win as early as 3 moves in, or as late as 5 moves in (full board)</li>
  <li>Lose is symmetric to win</li>
</ul>

<p><br /></p>

<p>The design is also dependent on the <strong>priority of the strategy</strong>, specifically:</p>

<ul>
  <li>Priority 1: Win</li>
  <li>Priority 2: Tie</li>
</ul>

<p><br /></p>

<p>The reason for having a small negative reward at every step is because the Tic-Tac-Toe game’s environment is set in a small 3 x 3 board, and even for a very smart agent, the later the game proceeds to, there are less moves for the agent to <strong>win</strong>. Even with a smart agent, the game will more likely result in a <strong>tie</strong> in a late game battle. Therefore, the negative reward at each step is to encourage the agent to finish the game as early as possible, which, in turn, increases its probability of winning the game.</p>

<p>The reward values for the <strong>win</strong> condition should be high enough to encourage good actions. The <strong>tie</strong> reward is the mid-point for <strong>win</strong> and <strong>lose</strong>, but also taking into account, if a game is played until the board is fully occupied (9 pieces), the sum of rewards for winning that late game should be the same as a tie. Since the agent always goes first, it can, at most, have 5 moves before the no moves left, this translates to \(-2 * 5 + 20 = 10\) episode reward. As mentioned before, we want to encourage the agent to win the game as early as possible. Therefore, <strong>winning a late-game is considered the same as tie, from a rewards perspective</strong>.</p>

<p><br /></p>

<h3 id="32-learning-algorithm">3.2 Learning Algorithm</h3>

<p>I chose to use Q-learning to solve this problem. This specific TIC-TAC-TOE game has a small fixed game board of \(3 \times 3\). Specifically, there are, at most, \(3^9 = 19683\) possible game states, in which there are 3 possible states (empty, X or O) for each of the \(3 \times 3 = 9\) positions. This can easily fit into the computer memory. Therefore, within a reasonable amount of training time, Q-learning is expected to find the global optimal policy. The discussion on how to determine whether the policy is a global optimal policy is in <strong>[4.2 Optimal Policy Discussion]</strong>.</p>

<p>The q-table is represented as a HashTable, specifically: <code class="language-plaintext highlighter-rouge">{"hashed_state": q-value}</code>. The <code class="language-plaintext highlighter-rouge">hashed_state</code> is just the game board converted into a string (i.e. <code class="language-plaintext highlighter-rouge">000000000</code>).</p>

<p><br /></p>

<h3 id="33-hyperparameters">3.3 Hyperparameters</h3>

<ul>
  <li><strong>Learning Rate (\(\alpha\))</strong>: 0.1</li>
  <li><strong>Exploration Rate (\(\epsilon\))</strong>: Initially 1, decreasing linearly with respect to episodes to 0</li>
  <li><strong>Discount Rate (\(\gamma\))</strong>: 0.9</li>
</ul>

<p><br /></p>

<h2 id="4-experiment--findings">4. Experiment &amp; Findings</h2>

<h3 id="41-training-evaluation">4.1 Training Evaluation</h3>

<p>The reward in the plot is the sum of rewards for every episode averaged over the last 20 episodes to reduce noise in the plot.</p>

<p><strong>Evaluating Trained Policy</strong></p>

<center>1000EP Training Session</center>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>EP100 | Agent Reward: -4
EP200 | Agent Reward: -4
EP300 | Agent Reward: 16
EP400 | Agent Reward: -6
EP500 | Agent Reward: 16
EP600 | Agent Reward: 14
EP700 | Agent Reward: 16
EP800 | Agent Reward: 16
EP900 | Agent Reward: 16
EP1000 | Agent Reward: 16
EP[1000] Avg Reard: 16.0
</code></pre></div></div>

<p><img src="/blog/assets/images/rl/tictactoe_p1_1000ep.png" alt="tictactoe_p1_1000ep" /></p>

<p><br /></p>

<center>500EP Training Session</center>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>EP100 | Agent Reward: -4
EP200 | Agent Reward: 16
EP300 | Agent Reward: 16
EP400 | Agent Reward: 16
EP500 | Agent Reward: 16
EP[500] Avg Reward: 14.9
</code></pre></div></div>

<p><img src="/blog/assets/images/rl/tictactoe_p1_500ep.png" alt="tictactoe_p1_500ep.png" /></p>

<p><br /></p>

<center>250EP Training Session</center>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>EP100 | Agent Reward: -6
EP200 | Agent Reward: 16
EP[250] Avg Reward: 14.0
</code></pre></div></div>

<p><img src="/blog/assets/images/rl/tictactoe_p1_250ep.png" alt="tictactoe_p1_250ep.png" /></p>

<p><br /></p>

<p><strong>Discussion</strong></p>

<p>From the three training instances, We can see that the 1000 EP agent, at the end of training, the final reward is above +10, and the test run shows 100% win rate against the opponent. However, there is still room for improvement for both the 250EP agent and 500EP agent compared to the 1000EP agent. Intuitively, this difference maybe due to the agent not able to <strong>win</strong> the game early on, resulting in a late game <strong>tie</strong>. The final reward for 1000EP agent is around 16, which is equivalent to winning the game in 3 steps (\(-2 * 2 + 20 * 1\)). The other two 500EP and 250EP agents only manages to reach around 14.9 and 14 episode reward respectively, which is probably converging to a local optimal of <strong>tie</strong> more than the <strong>win</strong> state.</p>

<p><br /></p>

<h3 id="42-optimal-policy-discussion">4.2 Optimal Policy Discussion</h3>

<p>A more direct way to determine whether the agent has learnt the globally optimal policy is to play the agent against itself. If the agent indeed learn the optimal policy, then both agents should reach a 100% <strong>tie</strong> rate.</p>

<p><strong>Notation:</strong> The agent that goes first will be denoted as <strong>Player 1</strong> and the agent that goes second will be denoted as <strong>Player 2</strong>.</p>

<p><br /></p>

<h4 id="experiment---play-two-copies-of-the-same-agent-against-each-other-by-following-the-trained-policy">Experiment - Play two copies of the same agent against each other, by following the trained policy</h4>

<p>By following a trained optimal deterministic strategy, after playing the trained agent against a copy of itself, <strong>Player 1</strong> always wins. This is because the policy <em>is trained on going first</em>, and will not perform well if used by <strong>Player 2</strong>.</p>

<p><br /></p>

<h4 id="experiment---train-two-agents-against-each-other-self-play">Experiment - Train two agents against each other (self-play)</h4>

<p>Note that I modified the reward structure for self-play. <strong>Win: +10, lose: -10, draw: 0, step: 0.</strong> The reward in the plot is the sum of rewards for every episode averaged over a moving 50 episodes to reduce noise in the plot. Based on the reward plot below, the 2 agents successfully converge to 0 reward and 100% tie rate. To verify that this is indeed a global optimal policy, refer to my next experiment, which runs the self-played agent against the original min-max agent.</p>

<p><img src="/blog/assets/images/rl/tictactoe_p1_5000ep_selfplay.png" alt="tictactoe_p1_5000ep_selfplay" /></p>

<p><img src="/blog/assets/images/rl/tictactoe_p2_5000ep_selfplay.png" alt="tictactoe_p2_5000ep_selfplay" /></p>

<p><img src="/blog/assets/images/rl/tictactoe_selfplay.png" alt="tictactoe_selfplay" /></p>

<p><br /></p>

<h4 id="experiment-re-evaluate-how-a-self-play-agent-player-1-performs-against-min-max-opponent">Experiment: Re-evaluate how a self-play agent (Player 1) performs against min-max opponent</h4>

<p>We would expect the self-played agents to converge to the global optimal policy, but it could be because the two agents are ‘colluding’ with each other and reaching the ‘tie’ state as the best outcome, similar to the prisoner’s dilemma. To verify this, we have to re-evaluate the self-played agent (Player 1) against the min-max opponent again to see. To verify my point, I intentionally ran two versions of self-play, (1) \(\textit{With}\) pre-trained policy that was trained against the min-max agent first, then self-play. (2) \(\textit{Without}\) pre-trained policy, the two agents self-play from scratch. Below are the outcomes after evaluating against the min-max opponent again. We can see that without prior knowledge, the self-play agent lost to the min-max agent. However, with prior training, the self-play agent won the min-max agent, thus verifying that it indeed reached the global optimal policy.</p>

<p><strong>Effects of Prior Training</strong></p>

<center>[Pre-trained against min-max opponent]</center>

<p><img src="/blog/assets/images/rl/tictactoe_selfplay_prior.png" alt="tictactoe_selfplay_prior" /></p>

<center>[No prior training against min-max opponent]</center>

<p><img src="/blog/assets/images/rl/tictactoe_selfplay_no_prior.png" alt="tictactoe_selfplay_no_prior" /></p>

<p><br /></p>

<h2 id="5-next-steps">5. Next Steps</h2>

<p><br /></p>

<p>Since this is a static game, we could extend this agent to play a more generic version of tic-tac-toe, with either a larger board size or with different mechanics (connecting more than 3 lines to win).</p>

<p>Since the reward function is defined in a very flexible way. There are potential opportunities to change the reward design to see if it has effects on the learning speed and agent performance.</p>

<p>This is the first step towards AlphaGo. There are way more complexities when it comes to mastering such a complicated game. But this is a taste of the capability of <strong>self-play</strong> in reinforcement learning.</p>

      </article>

      
        <div class="blog-tags">
          <span>Tags:</span>
          
            <a href="/blog/tags#reinforcement-learning">reinforcement-learning</a>
          
            <a href="/blog/tags#research">research</a>
          
        </div>
      

      

      
        <!-- Check if any share-links are active -->





      

      <ul class="pagination blog-pager">
        
        <li class="page-item previous">
          <a class="page-link" href="/blog/2019-12-03-acrobot/" data-toggle="tooltip" data-placement="top" title="OpenAI Gym - Acrobot-v1">&larr; Previous Post</a>
        </li>
        
        
        <li class="page-item next">
          <a class="page-link" href="/blog/2021-11-15-peaking-into-the-real-game/" data-toggle="tooltip" data-placement="top" title="Peaking into the real game">Next Post &rarr;</a>
        </li>
        
      </ul>
      
  
  
  

  


  



    </div>
  </div>
</div>


  <footer>
  <div class="container-md beautiful-jekyll-footer">
    <div class="row">
      <div class="col-xl-8 offset-xl-2 col-lg-10 offset-lg-1">
      <ul class="list-inline text-center footer-links"><li class="list-inline-item">
    <a href="mailto:hanxiangp@gmail.com" title="Email me">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fas fa-envelope fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">Email me</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://github.com/workofart" title="GitHub">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-github fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">GitHub</span>
   </a>
  </li><li class="list-inline-item">
    <a href="https://linkedin.com/in/pan-henry" title="LinkedIn">
      <span class="fa-stack fa-lg" aria-hidden="true">
        <i class="fas fa-circle fa-stack-2x"></i>
        <i class="fab fa-linkedin fa-stack-1x fa-inverse"></i>
      </span>
      <span class="sr-only">LinkedIn</span>
   </a>
  </li></ul>

      
      <p class="copyright text-muted">
      
        Henry Pan
        &nbsp;&bull;&nbsp;
      
      2021

      

      
      </p>
      <p class="theme-by text-muted">
        Powered by
        <a href="https://beautifuljekyll.com">Beautiful Jekyll</a>
      </p>
      </div>
    </div>
  </div>
</footer>


  
  
    
  <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=" crossorigin="anonymous"></script>


  
    
  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>


  
    
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>


  



  
    <!-- doing something a bit funky here because I want to be careful not to include JQuery twice! -->
    
      <script src="/blog/assets/js/beautifuljekyll.js"></script>
    
  





  
    
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


  





</body>
</html>
